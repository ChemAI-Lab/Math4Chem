{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systems of Nonlinear Equations\n",
    "\n",
    "This code is to help us visualize the solution of nonlinear linear equations. <br>\n",
    "\n",
    "This tutorial can be deployed in <a target=\"_blank\" href=\"https://colab.research.google.com/github/ChemAI-Lab/Math4Chem/blob/main/website/Lecture_Notes/Coding/nonlinear_equations.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Linear Equations\n",
    "\n",
    "We will start first with a review of linear regression but solved using an iterative search approach.<br>\n",
    "\n",
    "\n",
    "Here we define the a dummy linear model,\n",
    "$$\n",
    "f(\\mathbf{w},x) = \\begin{bmatrix}\n",
    "b, & a \\end{bmatrix}  \\begin{bmatrix}\n",
    "1 \\\\\n",
    " x \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "As we will see, the parameters of the linear model, $\\mathbf{w}^top=[a,b]$, are *free parameters*. Meaning, we usually use a metric function, commonly named to **loss function**, to search for the **optimal parameters**.<br>\n",
    "\n",
    ". The loss function measures the distance between the predicted values, $f(x,\\mathbf{w})$, and true values, $\\hat{y}$,. <br>\n",
    "\n",
    "## Least Squares\n",
    "As we saw in clasee, the least squares problem can be described using the matrix notation,\n",
    "$$\n",
    "\n",
    " {\\cal L}(\\mathbf{w}) =  \\frac{1}{2n}\\left ( \\mathbf{Y} - \\mathbf{X} \\mathbf{w} \\right )^\\top \\left ( \\mathbf{Y} - \\mathbf{X} \\mathbf{w} \\right )= \\frac{1}{2n} \\left (\\mathbf{Y}^\\top \\mathbf{Y} - 2\\mathbf{Y}^\\top\\mathbf{X} \\mathbf{w}  + \\mathbf{w}^\\top \\mathbf{X}^\\top\\mathbf{X} \\mathbf{w} \\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when deployed in Colab uncomment this line to install ipyml\n",
    "#!pip install ipympl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.tri as tri\n",
    "\n",
    "rng1 = np.random.default_rng(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2, 10)\n",
    "X = np.column_stack((np.ones_like(x), x))\n",
    "y = X@np.array([1, 2]) + rng1.uniform(-0.75, 0.75, size=x.shape)  # add some noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the values of a and b and see what happens\n",
    "a = 5\n",
    "b = 1\n",
    "w = np.array([b,a])\n",
    "\n",
    "x_grid = np.linspace(0, 2, 100)\n",
    "X_grid = np.column_stack((np.ones_like(x_grid), x_grid))\n",
    "y_grid = X_grid@w\n",
    "\n",
    "y_pred = X@w # calculate values of training data\n",
    "error = y_pred - y\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(11,5))\n",
    "fig.suptitle('Error in linear models',fontsize=18)\n",
    "\n",
    "# left panel \n",
    "ax1.plot(x_grid, y_grid, c='k')\n",
    "for xi, yi in zip(x, y):\n",
    "    y_pred_i = np.array([1,xi])@w\n",
    "    error_i = yi - y_pred_i\n",
    "    if error_i > 0:\n",
    "        ax1.vlines(xi, y_pred_i, y_pred_i + np.abs(error_i))\n",
    "    else:\n",
    "        ax1.vlines(xi, y_pred_i - np.abs(error_i), y_pred_i)\n",
    "ax1.text(0.1, .9, f'f(x) = {a} x  + {b}', fontsize=20, transform=ax1.transAxes)\n",
    "ax1.scatter(x, y, s=45)\n",
    "ax1.set_xlim(-0.5, 2.5)\n",
    "ax1.set_ylim(-2, 12)\n",
    "ax1.set_ylabel(r'$y$', fontsize=20)\n",
    "ax1.set_xlabel(r'$x$', fontsize=20)\n",
    "\n",
    "# right panel\n",
    "sum_errors = 0.\n",
    "for i,(xi, yi) in enumerate(zip(x, y)):\n",
    "    y_pred_i = np.array([1, xi])@w\n",
    "    error_i = yi - y_pred_i\n",
    "    sum_errors += error_i**2\n",
    "\n",
    "    if i == x.shape[0]-1:\n",
    "        ax2.scatter(xi, error_i**2, color='k', s=55,label=r'$\\epsilon^2(x_i)$')\n",
    "        ax2.scatter(xi, np.abs(error_i), color='r', s=55, marker='s',label=r'$|\\epsilon(x_i)|$')   \n",
    "    else:\n",
    "        ax2.scatter(xi, error_i**2, color='k', s=55)\n",
    "        ax2.scatter(xi, np.abs(error_i), color='r', s=55, marker='s')\n",
    "    \n",
    "\n",
    "ax2.set_ylabel(r'Error', fontsize=20)\n",
    "ax2.set_xlabel(r'$x$', fontsize=20)\n",
    "\n",
    "ax1.set_ylim(-0.2,12)\n",
    "plt.legend(loc=0, fontsize=18)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then we will proceed to compute the Jacobian of ${\\cal L}(\\mathbf{w})$ with respect to the parameters of the model, $\\mathbf{w}^\\top = [b,a]$,\n",
    "$$\n",
    " \\frac{\\partial }{\\partial \\mathbf{w}}{\\cal L}(\\mathbf{w}) = \\frac{1}{2n} \\frac{\\partial }{\\partial \\mathbf{w}}\\left (\\mathbf{Y}^\\top \\mathbf{Y} - 2\\mathbf{Y}^\\top\\mathbf{X} \\mathbf{w}  + \\mathbf{w}^\\top \\mathbf{X}^\\top\\mathbf{X} \\mathbf{w} \\right )\n",
    "$$\n",
    "which give us three unique terms,\n",
    "$$\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}} \\mathbf{Y}^\\top \\mathbf{Y} = \\mathbf{0} \\\\\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}} \\mathbf{Y}^\\top\\mathbf{X} \\mathbf{w} = \\mathbf{z} = \\mathbf{X}^\\top  \\mathbf{Y}\\\\\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}}\\mathbf{w}^\\top \\mathbf{X}^\\top\\mathbf{X} \\mathbf{w} = 2 \\mathbf{X}^\\top\\mathbf{X}\\mathbf{w}\n",
    "$$\n",
    "\n",
    "If we combine these three equations and we set the Jacobian to zero, we  get,\n",
    "$$\n",
    " \\frac{\\partial }{\\partial \\mathbf{w}}{\\cal L}(\\mathbf{w}) = \\frac{1}{n} \\left (- \\mathbf{X}^\\top  \\mathbf{Y}  + \\mathbf{X}^\\top\\mathbf{X} \\mathbf{w} \\right ),\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient\n",
    "def gradient_w(w0,x,y):\n",
    "    w = - x.T@y \n",
    "    w = w + x.T@x@w0\n",
    "    return w/x.shape[0]\n",
    "\n",
    "def loss_function(w0,x,y):\n",
    "    y_pred = x@w0\n",
    "    loss = 0.5 * np.mean((y_pred - y)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_solver(X, y):\n",
    "    Xt = X.T\n",
    "    A = Xt@X\n",
    "    z = Xt@y\n",
    "    A_inv = np.linalg.inv(A)\n",
    "    w = A_inv@z\n",
    "    return w  # optimal parameters\n",
    "\n",
    "w_opt = linear_model_solver(X,y)\n",
    "e_opt = loss_function(w_opt,X,y)\n",
    "print(w_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "Is a search method, used to find the minimum and maximum of functions using the gradient of a function.<br>\n",
    "The central point in gradient descent is **direction of largest decrease is** $-\\frac{\\partial f}{\\partial x}$ (the negative gradient of a function.)<br>\n",
    "\n",
    "The general equation for gradient descent is,\n",
    "$$\n",
    "\\mathbf{w}_{new} = \\mathbf{w} - \\eta \\nabla_{\\mathbf{w}} {\\cal L}(\\mathbf{w})\n",
    "$$\n",
    "where $\\eta$ is the learning rate, and $\\nabla_{\\mathbf{w}} {\\cal L}(\\mathbf{w})$ is the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_grid = np.linspace(-1, 3, 50)\n",
    "b_grid = np.linspace(-1, 3, 50)\n",
    "a_, b_ = np.meshgrid(a_grid, b_grid)\n",
    "w_ = np.column_stack((b_.flatten(), a_.flatten()))\n",
    "\n",
    "total_error_ = []\n",
    "for wi in w_:\n",
    "    ei = loss_function(wi, X, y)\n",
    "    total_error_.append(ei)\n",
    "total_error_ = np.array(total_error_)\n",
    "total_error_ = total_error_.reshape(a_.shape)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(11, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(b_, a_, total_error_, rstride=1, cstride=1,\n",
    "                cmap='viridis', edgecolor='none', alpha=0.5)\n",
    "\n",
    "eta = 0.01\n",
    "w_random = rng1.normal(-0.5, 0.5, size=(2,))\n",
    "# w_random = np.ones((2,))\n",
    "ax.set_title(r'$\\eta$ = ' + f'{eta:.2f}',fontsize=18)\n",
    "\n",
    "for i in range(10):\n",
    "    e = loss_function(w_random,X,y)\n",
    "    dw = gradient_w(w_random, X, y)\n",
    "    print(i,dw)\n",
    "    w_new = w_random - eta*dw\n",
    "    e_new = loss_function(w_new,X,y)\n",
    "    ax.quiver(w_random[0], w_random[1], e, w_new[0]-w_random[0],\n",
    "            w_new[1]-w_random[1], e_new-e)\n",
    "    ax.scatter(w_random[0], w_random[1], e, marker='x', s=55,color='k')\n",
    "    w_random = w_new\n",
    "\n",
    "ax.scatter(w_opt[0], w_opt[1], e_opt, marker='o', s=55, color='r')\n",
    "ax.set_xlabel('b', fontsize=25)\n",
    "ax.set_ylabel('a', fontsize=25)\n",
    "ax.set_zlabel('RMSE', fontsize=15, rotation=90)\n",
    "# ax.view_init(elev=30, azim=-120)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_grad_trajectory(eta,n_itr):\n",
    "    # w = np.random.normal(-5, 5, size=(2,))\n",
    "    w = np.random.uniform(-1, 2, size=(2,))\n",
    "    # w = np.ones((2,))\n",
    "    w_tr = w[None, :]\n",
    "    for i in range(n_itr):\n",
    "        dw = gradient_w(w, X, y)\n",
    "        e = loss_function(w, X, y)\n",
    "        w = w - eta * dw\n",
    "        w_tr = np.vstack((w_tr, w))\n",
    "    return w_tr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_grid = np.linspace(-1, 3, 50)\n",
    "b_grid = np.linspace(-1, 3, 50)\n",
    "b_, a_ = np.meshgrid(a_grid, b_grid)\n",
    "w_ = np.column_stack((b_.flatten(), a_.flatten()))\n",
    "\n",
    "total_error_ = []\n",
    "for wi in w_:\n",
    "    ei = loss_function(wi, X, y)\n",
    "    total_error_.append(ei)\n",
    "total_error_ = np.array(total_error_)\n",
    "total_error_ = total_error_.reshape(a_.shape)\n",
    "\n",
    "eta = 0.2\n",
    "n_itr = 10\n",
    "\n",
    "plt.figure(figsize=(11, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(b_, a_, total_error_, rstride=1, cstride=1,\n",
    "                cmap='viridis', edgecolor='none',alpha=0.5)\n",
    "\n",
    "# random initialization\n",
    "for i in range(10):\n",
    "    w_tr = get_grad_trajectory(eta,n_itr)\n",
    "    for wi in w_tr:\n",
    "        ax.scatter(wi[0], wi[1], loss_function(wi, X,y),\n",
    "            zorder=10, color='k', marker='x', s=15,linewidths=1.)  # + 1E-3 is only for display\n",
    "\n",
    "ax.scatter(w_opt[0], w_opt[1], e_opt, marker='o', s=55, color='r')\n",
    "ax.set_xlabel('b', fontsize=25)\n",
    "ax.set_ylabel('a', fontsize=25)\n",
    "ax.set_zlabel('RMSE', fontsize=15, rotation=90)\n",
    "# ax.view_init(elev=30, azim=-120)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the values of a and b and see what happens\n",
    "x_grid = np.linspace(0, 2, 100)\n",
    "X_grid = np.column_stack((np.ones_like(x_grid), x_grid))\n",
    "\n",
    "eta = 0.1\n",
    "n_itr = 100\n",
    "w_tr = get_grad_trajectory(eta,n_itr)\n",
    " \n",
    "fig, (ax1) = plt.subplots(1, 1, figsize=(9, 8))\n",
    "\n",
    "# left panel\n",
    "for i,wi in enumerate(w_tr):\n",
    "    y_pred = X_grid@wi\n",
    "    ax1.plot(x_grid, y_pred, alpha=0.75,label=f'{i}')\n",
    "\n",
    "ax1.plot(x_grid, X_grid@w_opt, c='k')\n",
    "ax1.scatter(x, y, c='k', s=45)\n",
    "\n",
    "ax1.set_ylabel(r'$y$', fontsize=20)\n",
    "ax1.set_xlabel(r'$x$', fontsize=20)\n",
    "\n",
    "# ax1.set_ylim(-0.2, 12)\n",
    "# plt.legend(loc=0, fontsize=18)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem3pc3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
