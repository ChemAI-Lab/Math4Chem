{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systems of Nonlinear Equations\n",
    "\n",
    "This code is to help us visualize the solution of nonlinear linear equations. <br>\n",
    "\n",
    "This tutorial can be deployed in <a target=\"_blank\" href=\"https://colab.research.google.com/github/ChemAI-Lab/Math4Chem/blob/main/website/Lecture_Notes/Coding/nonlinear_equations.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Linear Equations\n",
    "\n",
    "We will start first with a review of linear regression but solved using an iterative search approach.<br>\n",
    "\n",
    "\n",
    "Here we define the a dummy linear model,\n",
    "$$\n",
    "f(\\mathbf{w},x) = \\begin{bmatrix}\n",
    "b, & a \\end{bmatrix}  \\begin{bmatrix}\n",
    "1 \\\\\n",
    " x \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "As we will see, the parameters of the linear model, $\\mathbf{w}^top=[a,b]$, are *free parameters*. Meaning, we usually use a metric function, commonly named to **loss function**, to search for the **optimal parameters**.<br>\n",
    "\n",
    ". The loss function measures the distance between the predicted values, $f(x,\\mathbf{w})$, and true values, $\\hat{y}$,. <br>\n",
    "\n",
    "## Least Squares\n",
    "As we saw in clasee, the least squares problem can be described using the matrix notation,\n",
    "$$\n",
    " {\\cal L}(\\mathbf{w}) =  \\frac{1}{2n}\\left ( \\mathbf{Y} - \\mathbf{X} \\mathbf{w} \\right )^\\top \\left ( \\mathbf{Y} - \\mathbf{X} \\mathbf{w} \\right )= \\frac{1}{2n} \\left (\\mathbf{Y}^\\top \\mathbf{Y} - 2\\mathbf{Y}^\\top\\mathbf{X} \\mathbf{w}  + \\mathbf{w}^\\top \\mathbf{X}^\\top\\mathbf{X} \\mathbf{w} \\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when deployed in Colab uncomment this line to install ipyml\n",
    "#!pip install ipympl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.tri as tri\n",
    "\n",
    "rng1 = np.random.default_rng(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2, 10)\n",
    "X = np.column_stack((np.ones_like(x), x))\n",
    "y = X@np.array([1, 2]) + rng1.uniform(-0.75, 0.75, size=x.shape)  # add some noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the values of a and b and see what happens\n",
    "a = 5\n",
    "b = 1\n",
    "w = np.array([b,a])\n",
    "\n",
    "x_grid = np.linspace(0, 2, 100)\n",
    "X_grid = np.column_stack((np.ones_like(x_grid), x_grid))\n",
    "y_grid = X_grid@w\n",
    "\n",
    "y_pred = X@w # calculate values of training data\n",
    "error = y_pred - y\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(11,5))\n",
    "fig.suptitle('Error in linear models',fontsize=18)\n",
    "\n",
    "# left panel \n",
    "ax1.plot(x_grid, y_grid, c='k')\n",
    "for xi, yi in zip(x, y):\n",
    "    y_pred_i = np.array([1,xi])@w\n",
    "    error_i = yi - y_pred_i\n",
    "    if error_i > 0:\n",
    "        ax1.vlines(xi, y_pred_i, y_pred_i + np.abs(error_i))\n",
    "    else:\n",
    "        ax1.vlines(xi, y_pred_i - np.abs(error_i), y_pred_i)\n",
    "ax1.text(0.1, .9, f'f(x) = {a} x  + {b}', fontsize=20, transform=ax1.transAxes)\n",
    "ax1.scatter(x, y, s=45)\n",
    "ax1.set_xlim(-0.5, 2.5)\n",
    "ax1.set_ylim(-2, 12)\n",
    "ax1.set_ylabel(r'$y$', fontsize=20)\n",
    "ax1.set_xlabel(r'$x$', fontsize=20)\n",
    "\n",
    "# right panel\n",
    "sum_errors = 0.\n",
    "for i,(xi, yi) in enumerate(zip(x, y)):\n",
    "    y_pred_i = np.array([1, xi])@w\n",
    "    error_i = yi - y_pred_i\n",
    "    sum_errors += error_i**2\n",
    "\n",
    "    if i == x.shape[0]-1:\n",
    "        ax2.scatter(xi, error_i**2, color='k', s=55,label=r'$\\epsilon^2(x_i)$')\n",
    "        ax2.scatter(xi, np.abs(error_i), color='r', s=55, marker='s',label=r'$|\\epsilon(x_i)|$')   \n",
    "    else:\n",
    "        ax2.scatter(xi, error_i**2, color='k', s=55)\n",
    "        ax2.scatter(xi, np.abs(error_i), color='r', s=55, marker='s')\n",
    "    \n",
    "\n",
    "ax2.set_ylabel(r'Error', fontsize=20)\n",
    "ax2.set_xlabel(r'$x$', fontsize=20)\n",
    "\n",
    "ax1.set_ylim(-0.2,12)\n",
    "plt.legend(loc=0, fontsize=18)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then we will proceed to compute the Jacobian of ${\\cal L}(\\mathbf{w})$ with respect to the parameters of the model, $\\mathbf{w}^\\top = [b,a]$,\n",
    "$$\n",
    " \\frac{\\partial }{\\partial \\mathbf{w}}{\\cal L}(\\mathbf{w}) = \\frac{1}{2n} \\frac{\\partial }{\\partial \\mathbf{w}}\\left (\\mathbf{Y}^\\top \\mathbf{Y} - 2\\mathbf{Y}^\\top\\mathbf{X} \\mathbf{w}  + \\mathbf{w}^\\top \\mathbf{X}^\\top\\mathbf{X} \\mathbf{w} \\right )\n",
    "$$\n",
    "which give us three unique terms,\n",
    "$$\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}} \\mathbf{Y}^\\top \\mathbf{Y} = \\mathbf{0} \\\\\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}} \\mathbf{Y}^\\top\\mathbf{X} \\mathbf{w} = \\mathbf{z} = \\mathbf{X}^\\top  \\mathbf{Y}\\\\\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}}\\mathbf{w}^\\top \\mathbf{X}^\\top\\mathbf{X} \\mathbf{w} = 2 \\mathbf{X}^\\top\\mathbf{X}\\mathbf{w}\n",
    "$$\n",
    "\n",
    "If we combine these three equations and we set the Jacobian to zero, we  get,\n",
    "$$\n",
    " \\frac{\\partial }{\\partial \\mathbf{w}}{\\cal L}(\\mathbf{w}) = \\frac{1}{n} \\left (- \\mathbf{X}^\\top  \\mathbf{Y}  + \\mathbf{X}^\\top\\mathbf{X} \\mathbf{w} \\right ),\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient\n",
    "def gradient_w(w0,x,y):\n",
    "    w = - x.T@y \n",
    "    w = w + x.T@x@w0\n",
    "    return w/x.shape[0]\n",
    "\n",
    "def loss_function(w0,x,y):\n",
    "    y_pred = x@w0\n",
    "    loss = 0.5 * np.mean((y_pred - y)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_solver(X, y):\n",
    "    Xt = X.T\n",
    "    A = Xt@X\n",
    "    z = Xt@y\n",
    "    A_inv = np.linalg.inv(A)\n",
    "    w = A_inv@z\n",
    "    return w  # optimal parameters\n",
    "\n",
    "w_opt = linear_model_solver(X,y)\n",
    "e_opt = loss_function(w_opt,X,y)\n",
    "print(w_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "Is a search method, used to find the minimum and maximum of functions using the gradient of a function.<br>\n",
    "The central point in gradient descent is **direction of largest decrease is** $-\\frac{\\partial f}{\\partial x}$ (the negative gradient of a function.)<br>\n",
    "\n",
    "The general equation for gradient descent is,\n",
    "$$\n",
    "\\mathbf{w}_{new} = \\mathbf{w} - \\eta \\nabla_{\\mathbf{w}} {\\cal L}(\\mathbf{w})\n",
    "$$\n",
    "where $\\eta$ is the learning rate, and $\\nabla_{\\mathbf{w}} {\\cal L}(\\mathbf{w})$ is the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_grid = np.linspace(-1, 3, 50)\n",
    "b_grid = np.linspace(-1, 3, 50)\n",
    "a_, b_ = np.meshgrid(a_grid, b_grid)\n",
    "w_ = np.column_stack((b_.flatten(), a_.flatten()))\n",
    "\n",
    "total_error_ = []\n",
    "for wi in w_:\n",
    "    ei = loss_function(wi, X, y)\n",
    "    total_error_.append(ei)\n",
    "total_error_ = np.array(total_error_)\n",
    "total_error_ = total_error_.reshape(a_.shape)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(11, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(b_, a_, total_error_, rstride=1, cstride=1,\n",
    "                cmap='viridis', edgecolor='none', alpha=0.5)\n",
    "\n",
    "eta = 0.1\n",
    "w_random = rng1.normal(-0.5, 0.5, size=(2,))\n",
    "# w_random = np.ones((2,))\n",
    "ax.set_title(r'$\\eta$ = ' + f'{eta:.2f}',fontsize=18)\n",
    "\n",
    "for i in range(10):\n",
    "    e = loss_function(w_random,X,y)\n",
    "    dw = gradient_w(w_random, X, y)\n",
    "    print(i,dw)\n",
    "    w_new = w_random - eta*dw\n",
    "    e_new = loss_function(w_new,X,y)\n",
    "    ax.quiver(w_random[0], w_random[1], e, w_new[0]-w_random[0],\n",
    "            w_new[1]-w_random[1], e_new-e)\n",
    "    ax.scatter(w_random[0], w_random[1], e, marker='x', s=55,color='k')\n",
    "    w_random = w_new\n",
    "\n",
    "ax.scatter(w_opt[0], w_opt[1], e_opt, marker='o', s=55, color='r')\n",
    "ax.set_xlabel('b', fontsize=25)\n",
    "ax.set_ylabel('a', fontsize=25)\n",
    "ax.set_zlabel('RMSE', fontsize=15, rotation=90)\n",
    "# ax.view_init(elev=30, azim=-120)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_grad_trajectory(eta,n_itr):\n",
    "    # w = np.random.normal(-5, 5, size=(2,))\n",
    "    w = np.random.uniform(-1, 2, size=(2,))\n",
    "    # w = np.ones((2,))\n",
    "    w_tr = w[None, :]\n",
    "    for i in range(n_itr):\n",
    "        dw = gradient_w(w, X, y)\n",
    "        e = loss_function(w, X, y)\n",
    "        w = w - eta * dw\n",
    "        w_tr = np.vstack((w_tr, w))\n",
    "    return w_tr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_grid = np.linspace(-1, 3, 50)\n",
    "b_grid = np.linspace(-1, 3, 50)\n",
    "b_, a_ = np.meshgrid(a_grid, b_grid)\n",
    "w_ = np.column_stack((b_.flatten(), a_.flatten()))\n",
    "\n",
    "total_error_ = []\n",
    "for wi in w_:\n",
    "    ei = loss_function(wi, X, y)\n",
    "    total_error_.append(ei)\n",
    "total_error_ = np.array(total_error_)\n",
    "total_error_ = total_error_.reshape(a_.shape)\n",
    "\n",
    "eta = 0.2\n",
    "n_itr = 10\n",
    "\n",
    "plt.figure(figsize=(11, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(b_, a_, total_error_, rstride=1, cstride=1,\n",
    "                cmap='viridis', edgecolor='none',alpha=0.5)\n",
    "\n",
    "# random initialization\n",
    "for i in range(10):\n",
    "    w_tr = get_grad_trajectory(eta,n_itr)\n",
    "    for wi in w_tr:\n",
    "        ax.scatter(wi[0], wi[1], loss_function(wi, X,y),\n",
    "            zorder=10, color='k', marker='x', s=15,linewidths=1.)  # + 1E-3 is only for display\n",
    "\n",
    "ax.scatter(w_opt[0], w_opt[1], e_opt, marker='o', s=55, color='r')\n",
    "ax.set_xlabel('b', fontsize=25)\n",
    "ax.set_ylabel('a', fontsize=25)\n",
    "ax.set_zlabel('RMSE', fontsize=15, rotation=90)\n",
    "# ax.view_init(elev=30, azim=-120)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the values of a and b and see what happens\n",
    "x_grid = np.linspace(0, 2, 100)\n",
    "X_grid = np.column_stack((np.ones_like(x_grid), x_grid))\n",
    "\n",
    "eta = 0.1\n",
    "n_itr = 1000\n",
    "w_tr = get_grad_trajectory(eta,n_itr)\n",
    " \n",
    "fig, (ax1) = plt.subplots(1, 1, figsize=(9, 8))\n",
    "\n",
    "# left panel\n",
    "for i,wi in enumerate(w_tr[::5]):\n",
    "    y_pred = X_grid@wi\n",
    "    ax1.plot(x_grid, y_pred, alpha=0.75)\n",
    "\n",
    "ax1.plot(x_grid, X_grid@w_opt, c='k',label='optimal model')\n",
    "ax1.scatter(x, y, c='k', s=45, label='data',zorder=4)\n",
    "\n",
    "ax1.set_ylabel(r'$y$', fontsize=20)\n",
    "ax1.set_xlabel(r'$x$', fontsize=20)\n",
    "\n",
    "# ax1.set_ylim(-0.2, 12)\n",
    "plt.legend(loc=0, fontsize=18)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for NonLinear Equations\n",
    "\n",
    "Here, we will extend gradient descent for non linear equations. \n",
    "Let's do a simple example of the gradient descent for finding the minima of the following chemical equilibrium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    z = 2*np.log(2*x) - np.log(1-x) - np.log(1-3*x)\n",
    "    return z\n",
    "\n",
    "def df_dx(x):\n",
    "    dz = 4/(2*x)+1/(1-x)+9/(1-3*x)\n",
    "    return dz\n",
    "\n",
    "def error_function(x,k):\n",
    "    return (f(x) - k)**2\n",
    "\n",
    "def gradient_error_function(x,k):\n",
    "    return 2*(f(x)-k)*df_dx(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_k = np.log(4.34E-3)\n",
    "x_grid = np.linspace(0.11, 0.011, 1000)\n",
    "f_grid = f(x_grid)\n",
    "error_grid = error_function(x_grid,ln_k)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.clf()\n",
    "plt.plot(x_grid, f_grid, c='k', label='f(x)', ls='--')\n",
    "plt.hlines(ln_k, 0.011, 0.11, label='ln K')\n",
    "plt.plot(x_grid,error_grid, c='blue', label =r'${\\cal L}(x)$')\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('f(x)', fontsize=20)\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x0 = 0.01 # and \n",
    "x0 = 0.075\n",
    "eta = 1E-4\n",
    "ln_k = np.log(4.34E-3)\n",
    "x_trj = np.array(x0)\n",
    "f_trj = np.array(f(x0))\n",
    "for i in range(40):\n",
    "    dx0 = gradient_error_function(x0,ln_k)\n",
    "    e = error_function(x0, ln_k)\n",
    "    print(f'Iteration {i+1}, x={x0:.5f}, f(x)={f(x0):.5f}, e={e:.5f}')\n",
    "    x0 = x0 - eta * dx0\n",
    "    x_trj = np.append(x_trj, x0)\n",
    "    f_trj = np.append(f_trj, f(x0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = np.linspace(0.11, 0.011, 1000)\n",
    "error_grid = error_function(x_grid,ln_k)\n",
    "x_opt = x_grid[np.argmin(error_grid)]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.clf()\n",
    "plt.plot(x_grid, error_grid, c='blue',label=r'${\\cal L}(x)$')\n",
    "plt.vlines(x_opt,-8,8,ls=':',color='k',alpha=0.75)\n",
    "plt.scatter(x_trj, error_function(x_trj, ln_k), s=105, c='red',label='Gradient Descent')\n",
    "plt.scatter(x0, error_function(x0, ln_k), c='c', marker='x', s=100)\n",
    "plt.plot(x_grid, f_grid, c='k', label='f(x)', ls='--')\n",
    "plt.hlines(ln_k, 0.011, 0.11, label='ln K')\n",
    "# plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel('x',fontsize=20)\n",
    "plt.ylabel(r'${\\cal L}(x)$', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = np.linspace(0.11, 0.011, 1000)\n",
    "error_grid = error_function(x_grid, ln_k)\n",
    "x_opt = x_grid[np.argmin(error_grid)]\n",
    "\n",
    "\n",
    "f_trj = f(x_trj)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.clf()\n",
    "plt.scatter(x_trj, f_trj, s=105, c='red')\n",
    "\n",
    "plt.plot(x_grid, f_grid, c='k', label='f(x)', ls='--')\n",
    "plt.hlines(ln_k, 0.011, 0.11, label='ln K')\n",
    "# plt.xscale('log')\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel(r'$f(x)$', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_residuals(x,m0,k):\n",
    "    xa1 = x[0]\n",
    "    xa2 = x[1]\n",
    "    xw = x[2]\n",
    "    ka1 = k[0]\n",
    "    ka2 = k[1]\n",
    "    kw = k[2]\n",
    "    \n",
    "    g1 = ka1*(m0 + xa1) - (xa1 - xa2)*(xa1 + xa2 + xw)\n",
    "    g2 = ka2*(xa1 - xa2) - (xa2)*(xa1 + xa2 + xw)\n",
    "    gw = kw -xw*(xa1 + xa2 + xw)\n",
    "    return np.array([g1,g2,gw])\n",
    "\n",
    "def jacobian_f_r(x,m0,k):\n",
    "    xa1 = x[0]\n",
    "    xa2 = x[1]\n",
    "    xw = x[2]\n",
    "    ka1 = k[0]\n",
    "    ka2 = k[1]\n",
    "    kw = k[2]\n",
    "    \n",
    "    dg1_dxa1 = -ka1 - 2*xa1 - xw\n",
    "    dg1_dxa2 = 2*xa2 + xw\n",
    "    dg1_dxw = -xa1 + xa2\n",
    "    \n",
    "    dg2_dxa1 = ka2 - xa2\n",
    "    dg2_dxa2 = -ka2 + 2*xa2 - xa1 - xw\n",
    "    dg2_dxw = -xa2\n",
    "    \n",
    "    dg3_dxa1 = -xw\n",
    "    dg3_dxa2 = -xw\n",
    "    dg3_dxw = -xa1 - xa2 - 2*xw\n",
    "    \n",
    "    return np.array([[dg1_dxa1, dg1_dxa2, dg1_dxw], \n",
    "                     [dg2_dxa1, dg2_dxa2, dg2_dxw], \n",
    "                     [dg3_dxa1, dg3_dxa2, dg3_dxw]])\n",
    "\n",
    "def error_function(x0, m0, k):\n",
    "    g = f_residuals(x0,m0,k)\n",
    "    return 0.5 * np.dot(g,g)\n",
    "\n",
    "def gradient_error_function(x0, m0, k):\n",
    "    g = f_residuals(x0, m0, k)\n",
    "    jac_g = jacobian_f_r(x0, m0, k)\n",
    "    return jac_g.T @ g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.array([5.6E-2,1.5E-4,1E-14])\n",
    "m0 = 1E-4\n",
    "# x0 = np.array([9.8E-5,4.99E-5,6.67E-11])\n",
    "x0 = np.random.normal(-5,5,size=(3,))*1E-5\n",
    "\n",
    "print(f_residuals(x0, m0, k))\n",
    "print(error_function(x0, m0, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1E1\n",
    "x0_trj = np.array(x0[None,:])\n",
    "error_trj = np.array(error_function(x0, m0, k))\n",
    "dx0_norm_trj = np.array(np.linalg.norm(gradient_error_function(x0, m0,k)))\n",
    "for i in range(2000):\n",
    "    dx0 = gradient_error_function(x0, m0,k)\n",
    "    e = error_function(x0, m0,k)\n",
    "    print(f'Itr {i+1}, e = ', e , 'x = ',x0, '|dx| = ', np.linalg.norm(dx0))\n",
    "    x0 = x0 - eta * dx0\n",
    "    x0_trj = np.vstack((x0_trj, x0))\n",
    "    error_trj = np.append(error_trj, e)\n",
    "    dx0_norm_trj = np.append(dx0_norm_trj, np.linalg.norm(dx0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.clf()\n",
    "plt.plot(np.arange(error_trj.shape[0]),error_trj,label='error')\n",
    "plt.plot(np.arange(error_trj.shape[0]), dx0_norm_trj, label='norm gradient')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Iteration', fontsize=20)\n",
    "plt.ylabel('Error', fontsize=20)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x0_trj)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "xs = np.random.rand(100)\n",
    "ys = np.random.rand(100)\n",
    "zs = np.random.rand(100)\n",
    "ax.scatter(x0[0], x0[1], x0[2], c='k', marker='x',s=160)\n",
    "sc = ax.scatter(x0_trj[:, 0], x0_trj[:, 1], x0_trj[:, 2], c=error_trj, cmap='viridis')\n",
    "plt.colorbar(sc)\n",
    "ax.set_xlabel(r'$x_{a1}$', fontsize=18)\n",
    "ax.set_ylabel(r'$x_{a2}$', fontsize=18)\n",
    "ax.set_zlabel(r'$x_{w}$', fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton's method\n",
    "\n",
    "Newton's method is an alternative algorithm to solve non-linear equations of the from, $f(x) = 0$ or $f(x) = c$, where $c$ is a constant. <br>\n",
    "For our 1D system of chemical equilibrium, $f(x) = lnK$, which is similar to this style of problems.\n",
    "\n",
    "Each iteration of Newton's method is given by, \n",
    "$$\n",
    "x_{n+1} = x_n -\\left(\\frac{\\partial f(x)}{\\partial x} \\bigg|_{x_n}\\right)^{-1}\\left( f(x_n) - c\\right )$$\n",
    "\n",
    "We can start by initializing $x_0$ and then iteratively apply the Newton's method to find the root of the function.\n",
    "\n",
    "Here is the Python code for Newton's method for our 1D system of chemical equilibrium.<br>\n",
    "\n",
    "\n",
    "![Newton's Method Animation](https://github.com/ChemAI-Lab/Math4Chem/raw/main/website/Assigments/NewtonIteration_Ani.gif)<br>\n",
    "Animaiton of Newton's Method from [Wikipedia](https://en.wikipedia.org/wiki/Newton%27s_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    z = 2*np.log(2*x) - np.log(1-x) - np.log(1-3*x)\n",
    "    return z\n",
    "\n",
    "\n",
    "def df_dx(x):\n",
    "    dz = 4/(2*x)+1/(1-x)+9/(1-3*x)\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_step(x0,dx0,f0,f_value):\n",
    "    delta_f = f0 - f_value\n",
    "    x1 = x0 - delta_f/dx0\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 0.1\n",
    "ln_k = np.log(4.34E-3)\n",
    "\n",
    "x_trj = np.array(x0)\n",
    "f_trj = np.array(f(x0))\n",
    "for i in range(10):\n",
    "    f0 = f(x0)\n",
    "    dx0 = df_dx(x0) \n",
    "    x_trj = np.append(x_trj, x0)\n",
    "    f_trj = np.append(f_trj, f0)\n",
    "    print(i, x0)\n",
    "    x0 = newton_step(x0,dx0,f0,ln_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = np.linspace(0.11,0.021,1000)\n",
    "f_grid = f(x_grid)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.clf()\n",
    "plt.plot(x_grid,f_grid,c='k',label='f(x)',ls='--')\n",
    "plt.hlines(ln_k,0.021,0.11,label='ln K')\n",
    "plt.scatter(x_trj,f_trj,c='r',marker='o',label='Newton-Raphson')\n",
    "plt.plot(x_trj,f_trj,c='r')\n",
    "plt.xlabel('x',fontsize=20)\n",
    "plt.ylabel('f(x)', fontsize=20)\n",
    "plt.legend(fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton-Raphson to solve systems of $k$ equations\n",
    "We will start with $k=3$, meaning three simultaneous equilibria.<br>\n",
    "\n",
    "We are going to extend the Newton's method for a single function for these two simultaneous equilibria.<br>\n",
    "As we saw in class, when combined we get the following equation for each iteration,\n",
    "$$\n",
    "\\mathbf{x}_{n+1} = \\underbrace{\\begin{bmatrix}\n",
    "x^{(n)}_1\\\\\n",
    "x^{(n)}_2 \\\\\n",
    "x^{(n)}_3\n",
    "\\end{bmatrix}}_{\\mathbf{x}_{n}} - J_{\\mathbf{f}(\\mathbf{x}_n)}^{-1}\\begin{bmatrix}\n",
    "f_1(\\mathbf{x}_{n}) - \\ln K_1\\\\\n",
    "f_2(\\mathbf{x}_{n})- \\ln K_2 \\\\\n",
    "f_3(\\mathbf{x}_{n})- \\ln K_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where $J_{\\mathbf{f}(\\mathbf{x})}^{-1}$ is the inverse of the Jacobian matrix evaluated at the point $\\mathbf{x}_n$\n",
    "$$\n",
    "J_{\\mathbf{f}(\\mathbf{x}_n)} = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1(\\mathbf{x})}{\\partial x_1}|_{\\mathbf{x}_n} & \\frac{\\partial f_1(\\mathbf{x})}{\\partial x_2}|_{\\mathbf{x}_n}  & \\frac{\\partial f_1(\\mathbf{x})}{\\partial x_3}|_{\\mathbf{x}_n}\\\\\n",
    "\\frac{\\partial f_2(\\mathbf{x})}{\\partial x_2}|_{\\mathbf{x}_n} & \\frac{\\partial f_2(\\mathbf{x})}{\\partial x_2}|_{\\mathbf{x}_n} & \\frac{\\partial f_2(\\mathbf{x})}{\\partial x_3}|_{\\mathbf{x}_n} \\\\\n",
    "\\frac{\\partial f_3(\\mathbf{x})}{\\partial x_1}|_{\\mathbf{x}_n} & \\frac{\\partial f_3(\\mathbf{x})}{\\partial x_2}|_{\\mathbf{x}_n} & \\frac{\\partial f_3(\\mathbf{x})}{\\partial x_3}|_{\\mathbf{x}_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the three equilibrium, we can defined the following functions,\n",
    "$$\n",
    "\\mathbf{f}(\\mathbf{x}) = \\begin{bmatrix}\n",
    "f_1(\\mathbf{x})\\\\\n",
    "f_2(\\mathbf{x}) \\\\\n",
    "f_3(\\mathbf{x})\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "K_{a1}(m_0-x_{a1}) - (x_{a1}-x_{a2})(x_{a1}+ x_{a2} + x_{w}) \\\\\n",
    "K_{a2}(x_{a1} - x_{a2}) - (x_{a2})(x_{a1}+ x_{a2} + x_{w})\\\\\n",
    "K_{w} - (x_w)(x_{a1}+ x_{a2} + x_{w})\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0\\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The Jacobian of $\\mathbf{f}$ is, \n",
    "$$\n",
    "J_{\\mathbf{f}(\\mathbf{x})} = \n",
    "\\begin{bmatrix}\n",
    "-K_{a1}-2x_{a2}-x_w & 2x_{a2}+x_w  & -x_{a1} + x_{a2}  \\\\\n",
    "K_{a2}-x_{a2} & -K_{a2}-2x_{a2} - x_{a1} - x_w & -x_{a2} \\\\\n",
    "-x_w & -x_w & -x_{a1} - x_{a2} - x_{w}\n",
    " &  & \n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, m0, k):\n",
    "    xa1 = x[0]\n",
    "    xa2 = x[1]\n",
    "    xw = x[2]\n",
    "    ka1 = k[0]\n",
    "    ka2 = k[1]\n",
    "    kw = k[2]\n",
    "\n",
    "    g1 = ka1*(m0 + xa1) - (xa1 - xa2)*(xa1 + xa2 + xw)\n",
    "    g2 = ka2*(xa1 - xa2) - (xa2)*(xa1 + xa2 + xw)\n",
    "    gw = kw - xw*(xa1 + xa2 + xw)\n",
    "    return np.array([g1, g2, gw])\n",
    "\n",
    "\n",
    "def jacobian_f(x, m0, k):\n",
    "    xa1 = x[0]\n",
    "    xa2 = x[1]\n",
    "    xw = x[2]\n",
    "    ka1 = k[0]\n",
    "    ka2 = k[1]\n",
    "    kw = k[2]\n",
    "\n",
    "    dg1_dxa1 = -ka1 - 2*xa1 - xw\n",
    "    dg1_dxa2 = 2*xa2 + xw\n",
    "    dg1_dxw = -xa1 + xa2\n",
    "\n",
    "    dg2_dxa1 = ka2 - xa2\n",
    "    dg2_dxa2 = -ka2 + 2*xa2 - xa1 - xw\n",
    "    dg2_dxw = -xa2\n",
    "\n",
    "    dg3_dxa1 = -xw\n",
    "    dg3_dxa2 = -xw\n",
    "    dg3_dxw = -xa1 - xa2 - 2*xw\n",
    "\n",
    "    return np.array([[dg1_dxa1, dg1_dxa2, dg1_dxw],\n",
    "                     [dg2_dxa1, dg2_dxa2, dg2_dxw],\n",
    "                     [dg3_dxa1, dg3_dxa2, dg3_dxw]])\n",
    "\n",
    "\n",
    "def error_function(x0, m0, k):\n",
    "    g = f(x0, m0, k)\n",
    "    return 0.5 * np.dot(g, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_step_vector(x0, m0, k):\n",
    "    f_values = f(x0, m0, k)\n",
    "    jac_f = jacobian_f(x0, m0, k)\n",
    "    jac_f_inv = np.linalg.inv(jac_f)\n",
    "    x = x0 - jac_f_inv@(f_values)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.array([5.6E-2, 1.5E-4, 1E-14])\n",
    "ln_k = np.log(k)\n",
    "m0 = 1E-4\n",
    "x0 = np.array([9.8E-5, 4.99E-5, 6.67E-11])\n",
    "\n",
    "x_trj = x0[None,:]\n",
    "error_trj = np.array(error_function(x0, m0, k))\n",
    "for i in range(100):\n",
    "    x0 = newton_step_vector(x0, m0, k)\n",
    "    print(f'Iteration {i+1}: {x0}')\n",
    "    x_trj = np.vstack((x_trj, x0))\n",
    "    e = error_function(x0, m0, k)\n",
    "    error_trj = np.append(error_trj, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_trj.shape,error_trj.shape)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "xs = np.random.rand(100)\n",
    "ys = np.random.rand(100)\n",
    "zs = np.random.rand(100)\n",
    "ax.scatter(x0[0], x0[1], x0[2], c='k', marker='x', s=160)\n",
    "ax.plot(x_trj[:, 0], x_trj[:, 1],\n",
    "                x_trj[:, 2], c='k',alpha=0.5)\n",
    "sc = ax.scatter(x_trj[:, 0], x_trj[:, 1],\n",
    "                x_trj[:, 2], c=error_trj, cmap='viridis',s=150)\n",
    "plt.colorbar(sc)\n",
    "ax.set_xlabel(r'$x_{a1}$', fontsize=18)\n",
    "ax.set_ylabel(r'$x_{a2}$', fontsize=18)\n",
    "ax.set_zlabel(r'$x_{w}$', fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton-Raphson to solve systems of $k$ equations\n",
    "We will start with $k=3$, meaning thre simultaneous equilibria.\n",
    "$$\n",
    "\\mathbf{f}(\\mathbf{x}) = \\begin{bmatrix}\n",
    "f_1(\\mathbf{x})\\\\\n",
    "f_2(\\mathbf{x})\\\\\n",
    "f_3(\\mathbf{x})\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\ln K_1\\\\\n",
    "\\ln K_2 \\\\\n",
    "\\ln K_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\mathbf{x} = \\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "x_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "f_1(\\mathbf{x})\\\\\n",
    "f_2(\\mathbf{x})\\\\\n",
    "f_3(\\mathbf{x})\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\ln Q_1(\\mathbf{x})\\\\\n",
    "\\ln Q_2(\\mathbf{x}) \\\\\n",
    "\\ln Q_3(\\mathbf{x})\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to extend the Newton's method for a single function for these two simultaneous equilibria.<br>\n",
    "As we saw in class, when combined we get the following equation for each iteration,\n",
    "$$\n",
    "\\mathbf{x}_{n+1} = \\underbrace{\\begin{bmatrix}\n",
    "x^{(n)}_1\\\\\n",
    "x^{(n)}_2 \\\\\n",
    "x^{(n)}_3\n",
    "\\end{bmatrix}}_{\\mathbf{x}_{n}} - J_{\\mathbf{f}(\\mathbf{x}_n)}^{-1}\\begin{bmatrix}\n",
    "f_1(\\mathbf{x}_{n}) - \\ln K_1\\\\\n",
    "f_2(\\mathbf{x}_{n})- \\ln K_2 \\\\\n",
    "f_3(\\mathbf{x}_{n})- \\ln K_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where $J_{\\mathbf{f}(\\mathbf{x})}^{-1}$ is the inverse of the Jacobian matrix evaluated at the point $\\mathbf{x}_n$\n",
    "$$\n",
    "J_{\\mathbf{f}(\\mathbf{x}_n)} = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1(\\mathbf{x})}{\\partial x_1}|_{\\mathbf{x}_n} & \\frac{\\partial f_1(\\mathbf{x})}{\\partial x_2}|_{\\mathbf{x}_n}  & \\frac{\\partial f_1(\\mathbf{x})}{\\partial x_3}|_{\\mathbf{x}_n}\\\\\n",
    "\\frac{\\partial f_2(\\mathbf{x})}{\\partial x_2}|_{\\mathbf{x}_n} & \\frac{\\partial f_2(\\mathbf{x})}{\\partial x_2}|_{\\mathbf{x}_n} & \\frac{\\partial f_2(\\mathbf{x})}{\\partial x_3}|_{\\mathbf{x}_n} \\\\\n",
    "\\frac{\\partial f_3(\\mathbf{x})}{\\partial x_1}|_{\\mathbf{x}_n} & \\frac{\\partial f_3(\\mathbf{x})}{\\partial x_2}|_{\\mathbf{x}_n} & \\frac{\\partial f_3(\\mathbf{x})}{\\partial x_3}|_{\\mathbf{x}_n}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we will reuse the equations for the previous example, but instead of using $K=\\frac{x_1}{x_2}$, so that $f(x_1,x_2) = x_2 K - x_1$. <br>\n",
    " We will use $f(x_1,x_2) = \\ln K = \\ln(x_1) - \\ln(x_2)$.\n",
    "\n",
    " For the three prev. equilibrium, we get, \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\ln K_1\\\\\n",
    "\\ln K_2 \\\\\n",
    "\\ln K_3\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "f_1(\\mathbf{x}_{n})\\\\\n",
    "f_2(\\mathbf{x}_{n}) \\\\\n",
    "f_3(\\mathbf{x}_{n})\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\ln(x_{a1}-x_{a2}) + \\ln(x_{a1}+ x_{a2} + x_{w}) - \\ln(m_0-x_{a1})\\\\\n",
    "\\ln(x_{a2}) + \\ln(x_{a1}+ x_{a2} + x_{w}) - \\ln(x_{a1} - x_{a2})\\\\\n",
    "\\ln(x_{w}) + \\ln(x_{a1}+ x_{a2} + x_{w})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The Jacobian of $\\mathbf{f}$ is, \n",
    "$$\n",
    "J_{\\mathbf{f}(\\mathbf{x})} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{x_{a1} - x_{a2}} + \\frac{1}{x_{a1} + x_{a2} + x_w} + \\frac{1}{m_0 - x_{a1}} & -\\frac{1}{x_{a1} - x_{a2}} + \\frac{1}{x_{a1} + x_{a2} + x_w} & \\frac{1}{x_{a1} + x_{a2} + x_w} \\\\\n",
    "\\frac{1}{x_{a1} + x_{a2} + x_w} - \\frac{1}{x_{a1} - x_{a2}} & \\frac{1}{x_{a2}} + \\frac{1}{x_{a1} + x_{a2} + x_w} + \\frac{1}{x_{a1} - x_{a2}} & \\frac{1}{x_{a1} + x_{a2} + x_w}\\\\\n",
    "\\frac{1}{x_{a1} + x_{a2} + x_w} & \\frac{1}{x_{a1} + x_{a2} + x_w} & \\frac{1}{x_w} + \\frac{1}{x_{a1} + x_{a2} + x_w}\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will reuse the equations for the previous example\n",
    "def f(x, m0):\n",
    "    xa1 = x[0]\n",
    "    xa2 = x[1]\n",
    "    xw = x[2]\n",
    "    x_sum = np.sum(x)\n",
    "\n",
    "    f1 = np.log(xa1 - xa2) + np.log(x_sum) - np.log(m0 - xa1)\n",
    "    f2 = np.log(xa2) + np.log(x_sum) - np.log(xa1 - xa2)\n",
    "    fw = np.log(xw) + np.log(x_sum)\n",
    "    return np.array([f1, f2, fw])\n",
    "\n",
    "\n",
    "def jacobian_f(x, m0):\n",
    "    xa1 = x[0]\n",
    "    xa2 = x[1]\n",
    "    xw = x[2]\n",
    "    x_sum = np.sum(x)\n",
    "\n",
    "    df1_dxa1 = 1/(xa1-xa2) + 1/x_sum + 1/(m0-xa1)\n",
    "    df1_dxa2 = -1/(xa1-xa2) + 1/x_sum\n",
    "    df1_dxw = 1/x_sum\n",
    "\n",
    "    df2_dxa1 = 1/x_sum - 1/(xa1-xa2)\n",
    "    df2_dxa2 = 1/xa2 + 1/x_sum + 1/(xa1-xa2)\n",
    "    df2_dxw = 1/x_sum\n",
    "\n",
    "    df3_dxa1 = 1/x_sum\n",
    "    df3_dxa2 = 1/x_sum\n",
    "    df3_dxw = 1/xa2 + 1/x_sum\n",
    "\n",
    "    return np.array([[df1_dxa1, df1_dxa2, df1_dxw],\n",
    "                     [df2_dxa1, df2_dxa2, df2_dxw],\n",
    "                     [df3_dxa1, df3_dxa2, df3_dxw]])\n",
    "    \n",
    "def newton_step_vector(x0,m0,ln_k):\n",
    "    f_values = f(x0,m0)\n",
    "    jac_f = jacobian_f(x0,m0)\n",
    "    jac_f_inv = np.linalg.inv(jac_f)\n",
    "    print(jac_f_inv)\n",
    "    print(jac_f_inv)\n",
    "    print(f_values)\n",
    "    x = x0 - jac_f_inv@(f_values - ln_k)\n",
    "    # x = np.maximum(1E-14,x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.array([5.6E-2, 1.5E-4, 1E-14])\n",
    "ln_k = np.log(k)\n",
    "m0 = 2E4\n",
    "x0 = np.array([9.8E-5,4.99E-5,6.67E-11])\n",
    "\n",
    "for i in range(10):\n",
    "    x0 = newton_step_vector(x0, m0, ln_k)\n",
    "    print(f'Iteration {i+1}: {x0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem3pc3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
