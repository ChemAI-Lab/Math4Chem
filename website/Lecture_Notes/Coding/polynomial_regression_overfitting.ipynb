{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with a polynomial expansion\n",
    "\n",
    "This code is to help us visualize the linear models for polynomials. <br>\n",
    "\n",
    "This tutorial can be deployed in <a target=\"_blank\" href=\"https://colab.research.google.com/github/ChemAI-Lab/Math4Chem/blob/main/website/Lecture_Notes/Coding/polynomial_regression_overfitting.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.tri as tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random data over f(x) = sin(x) + x - 1\n",
    "def get_data(N, bool_biased=True):\n",
    "    # This creates an array x of N linearly spaced values between -1 and 1.\n",
    "    x = np.linspace(-1., 1., N) + np.random.uniform(low=-.1, high=.1, size=N)\n",
    "    y = 1.2*np.sin(2*x) + x - 1.\n",
    "    # Adds random noise to each y value.\n",
    "    y = y + np.random.uniform(low=-.35, high=.35, size=x.shape)\n",
    "    if bool_biased:\n",
    "        X = np.column_stack((np.ones_like(x), x))\n",
    "    else:\n",
    "        X = x[:, None]\n",
    "    return X, y\n",
    "\n",
    "X, y = get_data(10, bool_biased=True)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact Solution of Linear Regression ##\n",
    "\n",
    "* **Gradient of a function equal to zero means a maxima or minima**\n",
    "  \n",
    "$$\n",
    "    \\nabla {\\cal L}(\\mathbf{w}) \\Big\\rvert_{\\mathbf{w}^{*}} = \\frac{1}{2n} \\nabla_{\\mathbf{w}} \\left [ \\left (\\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right)^\\top \\left (\\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right) \\right ]= 0\n",
    "$$\n",
    "\n",
    "To solve for $\\mathbf{w}^*$, let's expand $ \\left (\\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right)^\\top \\left (\\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right)$,\n",
    "\n",
    "$$\n",
    "    \\left (\\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right)^\\top \\left (\\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right) = \\mathbf{y}^\\top \\mathbf{y}  - \\mathbf{y}^\\top \\mathbf{X}\\mathbf{w} -  \\mathbf{w}^\\top\\mathbf{X}^\\top\\mathbf{y} +   \\mathbf{w}^\\top\\mathbf{X}^\\top \\mathbf{X}\\mathbf{w}\n",
    "$$\n",
    "$$\n",
    "    \\nabla_{\\mathbf{w}} {\\cal L}(\\mathbf{w}) = \\frac{1}{2n}\\left(  -2 \\mathbf{X}^\\top\\mathbf{y} + 2\\mathbf{X}^\\top\\mathbf{X}\\mathbf{w} \\right) = 0\n",
    "$$\n",
    "\n",
    "## Optimal parameters ##\n",
    "\n",
    "$$\n",
    " \\mathbf{w}^* = \\left ( \\mathbf{X}^\\top \\mathbf{X} \\right ) ^{-1} \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "**Extra:**\n",
    "1. Homework, Proof the above equations.\n",
    "2. [Equations from Sections 2.4.1 and 2.4.2](https://www2.imm.dtu.dk/pubdb/edoc/imm3274.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trining a linear model\n",
    "def linear_model_solver(X, y):\n",
    "    Xt = X.T\n",
    "    A = Xt@X\n",
    "    z = Xt@y\n",
    "    A_inv = np.linalg.inv(A)\n",
    "    w = A_inv@z\n",
    "    return w  # optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the optimal parameters\n",
    "w_opt = linear_model_solver(X, y)\n",
    "\n",
    "y_data_pred = X@w_opt\n",
    "r2_linear_model = r2_score(y,y_data_pred)\n",
    "print('R-squared score: ', r2_linear_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot our model\n",
    "X_grid = np.linspace(-1.1,1.1,100)\n",
    "X_grid = np.column_stack((np.ones_like(X_grid),X_grid) )\n",
    "y_pred = X_grid@w_opt\n",
    "\n",
    "# plot the data\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "ax.text(0.05,0.8,r'$R^2 = $' + f'{r2_linear_model:0.3f}',transform=ax.transAxes,fontsize=20)\n",
    "ax.scatter(X[:, 1], y, label='data', s=75)\n",
    "ax.plot(X_grid[:, 1], y_pred, c='k',label='model prediction', markersize=5)\n",
    "ax.set_xlabel('x',fontsize=18)\n",
    "ax.set_ylabel('f(x)', fontsize=18)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond Linear Models\n",
    "\n",
    "Let's revise polynomials:\n",
    "\n",
    "How many terms if we have a second-order polynomial and $d=3$?\n",
    "$$\n",
    "\\begin{align}\n",
    "(1+x_1+x_2+x_3)^3 &= (1+x_1+x_2+x_3)(1+x_1+x_2+x_3)^2 \\\\\n",
    "&= 1+3x_1+3x^2_1+x^3_1+3x_2+6x_1x_2+3x^2_1x_2 \\\\ \n",
    "& +3x_2^2+3x_1x_2^2+x_2^3 +3x_3+6x_1x_3+3x_1^2x_3 \\\\\n",
    "& +6x_2x_3+6x_1x_2x_3+3x_2^2x_3+3x_3^2 \\\\\n",
    "& +3x_1x_3^2+3x_2x_3^2+x_3^3\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This is simply a new representation of $x$\n",
    "$$\n",
    "\\phi(\\mathbf{x}) = [1, x_1, x_2, x_3, \\cdots, x_i x_j, \\cdots, x_i^{ m} x_j^{p}, \\cdots, x_i^{ m} x_j^{p}x_{\\ell}^{r}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear models on basis-set expansion**\n",
    "\n",
    "$$\n",
    "    f(\\mathbf{x},\\mathbf{w}) = \\sum_{i=0}^d w_i \\phi(\\mathbf{x}) = \\mathbf{w}^\\top \\phi(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "\n",
    "* **Loss function**,\n",
    "$$\n",
    "    \\begin{align}\n",
    "    {\\cal L}(\\mathbf{w}) &= \\frac{1}{2n}\\sum_i^N (y_i - f(\\mathbf{x}_i,\\mathbf{w}))^2 = \\frac{1}{2}\\sum_i^N (y_i - \\mathbf{w}^\\top \\phi(\\mathbf{x}_i))^2 \\\\\n",
    "    &= \\frac{1}{2n} \\left (\\mathbf{y} - \\Phi(\\mathbf{x})\\mathbf{w} \\right)^\\top \\left (\\mathbf{y} -  \\Phi(\\mathbf{x})\\mathbf{w} \\right)\n",
    "    \\end{align}\n",
    "$$\n",
    "Homework, proof the above equations.\n",
    "\n",
    "\n",
    "1. What is $\\Phi(\\mathbf{x})$?\n",
    "2. What is the form of the **optimal** parameters $\\mathbf{w}^*$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of coding how to create the polynomial representation, we will use Sklearn. <br>\n",
    "Read the documentation: [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(X, p):\n",
    "    # transform the input data to polynomial features up to degree p\n",
    "    poly = PolynomialFeatures(p)\n",
    "    Phi = poly.fit_transform(X)\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.ones((1,1))\n",
    "x = np.random.normal(-1.,0.1,size=(1,1))\n",
    "print(f'Original x \\n{x}')\n",
    "for p in range(1,4):\n",
    "    x_phi = polynomial_features(x,p)\n",
    "    print(f'For degree {p}, the polynomial features are:\\n{x_phi}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the training data to polynomial features\n",
    "p = 3\n",
    "\n",
    "X = X[:,-1:] # select the last column\n",
    "print(X)\n",
    "X_phi = polynomial_features(X, p)\n",
    "print(X)\n",
    "print(X_phi)\n",
    "w_phi_opt = linear_model_solver(X_phi, y)\n",
    "print(f'Optimal parameters\\n{w_phi_opt}')\n",
    "\n",
    "y_data_phi_pred = X_phi@w_phi_opt\n",
    "r2_score_poly = r2_score(y, y_data_phi_pred)\n",
    "print(f'R-squared score for the polynomial model: {r2_score_poly}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_grid = np.linspace(-1.25,1.25,100)\n",
    "X_grid = X_grid[:,None]\n",
    "\n",
    "X_grid_phi = polynomial_features(X_grid, p)\n",
    "y_pred_phi = X_grid_phi@w_phi_opt\n",
    "\n",
    "# plot the data and the polynomial model\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "ax.text(0.05, 0.75, r'$R^2_{\\text{poly}} = $' +\n",
    "        f'{r2_score_poly:0.3f}', transform=ax.transAxes, fontsize=20)\n",
    "ax.scatter(X[:, 0], y, label='data', s=75)\n",
    "ax.plot(X_grid[:, 0], y_pred, c='red', ls = '--',\n",
    "        label='linear model ' + r'$R^2 = $' +\n",
    "        f'{r2_linear_model:0.3f}', markersize=5)\n",
    "ax.plot(X_grid[:, 0], y_pred_phi, c='k',label=f'polynomial model (p={p})', markersize=5)\n",
    "ax.set_xlabel('x',fontsize=18)\n",
    "ax.set_ylabel('f(x)', fontsize=18)\n",
    "plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 as a function of the polynomial degree\n",
    "p = 4\n",
    "X = X[:, -1:]  # select the last column\n",
    "\n",
    "X_grid = np.linspace(-1.1, 1.1, 100)\n",
    "X_grid = X_grid[:, None]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "ax.scatter(X[:, 0], y, label='data', s=105,zorder=4)\n",
    "ax.plot(X_grid[:, 0], y_pred, c='red', ls='--',\n",
    "        label='p=1' + r'$R^2 = $' +\n",
    "        f'{r2_linear_model:0.3f}', markersize=5)\n",
    "    \n",
    "for p in range(2,11,1):\n",
    "\n",
    "    X_phi = polynomial_features(X, p)\n",
    "    w_phi_opt = linear_model_solver(X_phi, y)\n",
    "    # print(f'Optimal parameters\\n{w_phi_opt}')\n",
    "\n",
    "    y_data_phi_pred = X_phi@w_phi_opt\n",
    "    r2_score_poly = r2_score(y, y_data_phi_pred)\n",
    "    print(f'degree: {p}, R2= {r2_score_poly}')\n",
    "\n",
    "    X_grid_phi = polynomial_features(X_grid, p)\n",
    "    y_pred_phi = X_grid_phi@w_phi_opt\n",
    "# plot the data and the polynomial model\n",
    "\n",
    "    ax.plot(X_grid[:, 0], y_pred_phi,\n",
    "            label=f'p={p} ' +  r' $R^2 = $' +\n",
    "            f'{r2_score_poly:0.3f}',  markersize=5, alpha=0.95*p/15)\n",
    "ax.set_xlabel('x', fontsize=18)\n",
    "ax.set_ylabel('f(x)', fontsize=18)\n",
    "plt.legend(fontsize=8,loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenge of Overfitting\n",
    "\n",
    "- **Risk in Flexibility**:  Higher-degree polynomials might fit the training data too precisely, capturing noise rather than the underlying trend, a phenomenon known as overfitting.\n",
    "- **Consequences**: An overfitted model performs poorly on new, unseen data, rendering it less effective for predictive purposes.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    ">#### **Learn More:**\n",
    "1. **What is Overfitting in Deep Learning**: [Educational Resource](https://www.v7labs.com/blog/overfitting)\n",
    "2. **Regularization in Machine Learning (with Code Examples)**: [Example Application](https://www.dataquest.io/blog/regularization-in-machine-learning/)\n",
    "3. **Polynomial Regression**: [Educational Resource](https://medium.com/analytics-vidhya/polynomial-regression-%EF%B8%8F-e0e20bfbe9d5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem3pc3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
