{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhq4H6VMfJU7",
    "outputId": "5cdc6b37-78b4-4f94-efa8-34d3393a9884"
   },
   "outputs": [],
   "source": [
    "#!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ywr4UzTFMns3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.functional as F\n",
    "from torchviz import make_dot\n",
    "import hiddenlayer as hl\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlmObDC0aVkz"
   },
   "source": [
    "\n",
    "# Introduction to Neural Networks.\n",
    " <a target=\"_blank\" href=\"https://colab.research.google.com/github/ChemAI-Lab/Math4Chem/blob/main/website/Lecture_Notes/Coding/Introduction_to_neural_networks.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "## **Abstract**\n",
    "\n",
    "1. **Introduction to Activation Functions.**: Activation functions in neural networks, such as sigmoid, ReLU, and softmax, apply non-linear transformations to inputs, enabling the network to capture complex data patterns and behaviors.\n",
    "\n",
    "2. **Introduction to Non-Linear Layers**: Non-linear layers in neural networks transform linear inputs into non-linear outputs using activation functions, enabling the network to handle complex, non-linear relationships in data.\n",
    "\n",
    "\n",
    ">### **References: Essential Resources for Further Learning**\n",
    ">\n",
    ">- **PyTorch**: [Official Documentation](https://pytorch.org/docs/stable/index.html)\n",
    ">- **Activation Functions in Neural Networks**: [Online Course](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)\n",
    ">- **Activation Functions in Neural Networks [12 Types & Use Cases]**: [Blog](https://www.v7labs.com/blog/neural-networks-activation-functions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lwb_Op0tNTXk"
   },
   "source": [
    "## Beyond Linear Models\n",
    "\n",
    "In extending the linear model to a **non-linear model**, the function $f(\\mathbf{x})$ becomes dependent on both external and internal parameters. This can be dissected as follows:\n",
    "\n",
    "- **Non-Linear Model Representation**:\n",
    "  - The model is now expressed as $f(\\mathbf{x}) = \\mathbf{w}^\\top \\phi(\\mathbf{x},\\mathbf{w}') = \\sum_i w_i \\phi_i(\\mathbf{x},\\mathbf{w}')$.\n",
    "  - In this representation, $\\phi(\\mathbf{x},\\mathbf{w}')$ indicates a non-linear map to a new **feature representation**, which also depends on internal parameters $\\mathbf{w}'$.\n",
    "\n",
    "- **Internal Parameter Optimization**:\n",
    "  - The model includes optimization of the non-linear parameters $\\mathbf{w}'$.\n",
    "  - This introduces an additional layer of complexity compared to the linear model, as $\\mathbf{w}'$ need to be adjusted along with $\\mathbf{w}$.\n",
    "\n",
    "Let's assume $\\phi(\\mathbf{x},\\mathbf{w}')$ is another linear model,\\\n",
    "$\\phi(\\mathbf{x},\\mathbf{w}') = \\mathbf{z} = [z_0,z_1,\\cdots,z_\\ell]$, where $\\ell$ is the \"new\" number of features.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kf_kqzG3R_Um"
   },
   "outputs": [],
   "source": [
    "# Function\n",
    "def f(x):\n",
    " return -(1.4 - 3.0 * x) * torch.sin(18.0 * x)\n",
    "\n",
    "def get_data(n_batch=200):\n",
    "    # X = torch.randn((n_batch,1))\n",
    "    X = torch.distributions.uniform.Uniform(-0.01,1.).sample([n_batch,1])\n",
    "    y = f(X)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O42yJ6yaZkpH",
    "outputId": "fe3e146d-9b6f-4bf6-adf7-7bffeccd3b18"
   },
   "outputs": [],
   "source": [
    "# Model definition\n",
    "\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(1, 100),\n",
    "  nn.Linear(100, 75),\n",
    "  nn.Linear(75, 1)\n",
    "    )\n",
    "for parameter in model.parameters():\n",
    "    print(parameter.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O9ZoMzDjZ3V5",
    "outputId": "9bc271ed-615c-4a3e-c201-adaa1a9ba67b"
   },
   "outputs": [],
   "source": [
    "layer_1 = nn.Linear(1, 100) # 1 to 100\n",
    "layer_2 = nn.Linear(100, 75) # 100 to 75\n",
    "layer_3 = nn.Linear(75, 1) # 75 to 1\n",
    "\n",
    "x = torch.randn(1,1)\n",
    "z1 = layer_1(x)\n",
    "z2 = layer_2(z1)\n",
    "z3 = layer_3(z2)\n",
    "\n",
    "print('layer 1', z1.shape)\n",
    "print('layer 2', z2.shape)\n",
    "print('layer 3', z3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "w1RVK_IHfdHl",
    "outputId": "0af9e222-0fa8-401d-fb6a-bda777b2583a"
   },
   "outputs": [],
   "source": [
    "y = model(x)\n",
    "make_dot(y, params=dict(model.named_parameters())).render(\"mlp\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mF2jNpyBSCJd"
   },
   "outputs": [],
   "source": [
    "  # Training function\n",
    "def training(model, training_iter=500, n_batch=50, lr=0.05, feedback_interval=100):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model.train()\n",
    "\n",
    "    for itr in range(1, training_iter + 1):\n",
    "        X, y_true = get_data(n_batch)\n",
    "        output = model(X)\n",
    "        loss_val = loss_fn(output, y_true)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step() #gradient step\n",
    "\n",
    "        if itr % feedback_interval == 0:\n",
    "            print(f'Iteration = {itr}, Loss = {loss_val.item():.4f}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OyjXc6K_R8tH"
   },
   "outputs": [],
   "source": [
    "def plot_model_performance(model, n_samples=75, n_grid_points=5000):\n",
    "    plt.clf()\n",
    "    X, y = get_data(n_samples)\n",
    "    X_grid = torch.linspace(0., 1., n_grid_points).unsqueeze(1)\n",
    "\n",
    "    plt.scatter(X.detach().numpy(), y.detach().numpy(), label='Batch')\n",
    "    plt.plot(X_grid.detach().numpy(), f(X_grid).detach().numpy(), ls='--', c='k', label=r'$f(x)$')\n",
    "    plt.plot(X_grid.detach().numpy(), model(X_grid).detach().numpy(), c='red', label=r'$NN(x)$')\n",
    "\n",
    "    plt.ylabel(r'$f(x)$', fontsize=12)\n",
    "    plt.xlabel(r'$x$', fontsize=12)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pHbJLlm9SPef",
    "outputId": "b01afc22-61b4-4536-8bcf-635a7462ecd4"
   },
   "outputs": [],
   "source": [
    "model_trained = training(model)\n",
    "plot_model_performance(model_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VTk3B-7TEfG"
   },
   "source": [
    "### **Non-Linear Layers**\n",
    "\n",
    "Consider the composition of two linear models:\n",
    "\n",
    "$$\n",
    "f(x,\\{\\mathbf{W}\\}_{\\ell=1}^{2}) = \\mathbf{W}_2^\\top (\\mathbf{W}^\\top_1\\mathbf{x}) = \\mathbf{W}^\\top_2 \\mathbf{z}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "In this expression, $\\mathbf{W}_1$ and $\\mathbf{W}_2$ are the weight matrices of the two linear models. The function $f(x,\\{\\mathbf{W}\\}_{\\ell=1}^{2})$ can be rewritten to highlight the composition of these models:\n",
    "\n",
    "$$\n",
    "f(x,\\{\\mathbf{W}\\}_{\\ell=1}^{2}) = \\mathbf{W}_2^\\top \\phi(\\mathbf{W}_1, \\mathbf{x})\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Where the function $\\phi(\\mathbf{W}_1, \\mathbf{x})$ is defined as the product of the input $\\mathbf{x}$ and the transpose of the first weight matrix $\\mathbf{W}_1$:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\phi(\\mathbf{W}_1, \\mathbf{x}) = \\mathbf{x}\\mathbf{W}^\\top_1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t75NXzN8Tl1H"
   },
   "source": [
    "## **Introducting Activation functions**\n",
    "\n",
    "Let's revisit the structure of Nueral Networks to understand activation functions:\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <style>\n",
    "        .centered-image {\n",
    "            display: block;\n",
    "            margin-left: auto;\n",
    "            margin-right: auto;\n",
    "            width: 50%;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<a href=\"https://www.geeksforgeeks.org/activation-functions/\" target=\"_blank\">\n",
    "    <img src=\"https://media.geeksforgeeks.org/wp-content/cdn-uploads/33-1-1.png\"\n",
    "         alt=\"Varied Initial Conditions for Gradient Descent\"\n",
    "         class=\"centered-image\">\n",
    "</a>\n",
    "\n",
    "<br>\n",
    "<figcaption align = \"center\"><b>Figure 1 - Activation Functions. Figure by\n",
    "Vineet Joshi.</b></figcaption>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "<br>\n",
    "\n",
    "An **activation function** (also known as \"transfer functions\") in a neural networks define how the weighted sum of the input (**Wnj**) is transformed into an output from a node or nodes in a layer of the network.\n",
    "\n",
    "<br>\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <style>\n",
    "        .centered-image {\n",
    "            display: block;\n",
    "            margin-left: auto;\n",
    "            margin-right: auto;\n",
    "            width: 50%;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<a href=\"https://prabhakar-rangarao.medium.com/activation-functions-9020acfa80b6\" target=\"_blank\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1358/1*uUzr1DsZm5P6IqRXIlpfhQ.gif\"\n",
    "         alt=\"Activation Functions Animated\"\n",
    "         class=\"centered-image\">\n",
    "</a>\n",
    "\n",
    "<br>\n",
    "<figcaption align = \"center\"><b>Figure 2 - Activation Functions Animated. Figure by\n",
    "Prabhakar Rangarao.</b></figcaption>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "<br>\n",
    "\n",
    "Typically, a differentiable nonlinear activation function is used in the hidden layers of a neural network. This allows the model to learn more complex functions than a network trained using a linear activation function.\n",
    "\n",
    "<br>\n",
    "\n",
    "* Hyperbolic tangent\n",
    "$$\n",
    "tanh(x) = \\frac{\\exp(x)-\\exp(-x)}{\\exp(x)+\\exp(-x)}\n",
    "$$\n",
    "\n",
    "* Sigmoid\n",
    "$$\n",
    "\\text{sigmoid}(x_i) = \\frac{1}{1+ \\exp(-x)}\n",
    "$$\n",
    "\n",
    "* ReLU\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0,x)\n",
    "$$\n",
    "\n",
    "* Leaky RLU\n",
    "$$\n",
    "\\text{LeakyReLU}(x) = \\max(0,x) + \\beta*\\min(0,x)\n",
    "$$\n",
    "\n",
    "* SiLU\n",
    "$$\n",
    "\\text{SiLU}(x) = x * \\sigma(x)\\\\\n",
    "\\sigma(x) = \\frac{1}{1+\\exp(-x)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFuAG5vGYrAM"
   },
   "outputs": [],
   "source": [
    "x = torch.linspace(-5,5,1000)\n",
    "\n",
    "act_tanh = nn.Tanh()\n",
    "y_tanh = act_tanh(x)\n",
    "\n",
    "act_sigmoid = nn.Sigmoid()\n",
    "y_sigmoid = act_sigmoid(x)\n",
    "\n",
    "act_relu = nn.ReLU()\n",
    "y_relu = act_relu(x)\n",
    "\n",
    "act_lrelu = nn.LeakyReLU(0.1)\n",
    "y_lrelu = act_lrelu(x)\n",
    "\n",
    "act_silu = nn.SiLU()\n",
    "y_silu = act_silu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "knZdWa0vYriB",
    "outputId": "5567c222-aa4f-426f-f74e-2d58a8f6289a"
   },
   "outputs": [],
   "source": [
    "xnp = x.detach().numpy()\n",
    "plt.plot(xnp,y_tanh.detach().numpy(),label='Tanh')\n",
    "plt.plot(xnp,y_sigmoid.detach().numpy(),label='Sigmoid')\n",
    "plt.plot(xnp,y_relu.detach().numpy(),label='ReLU')\n",
    "plt.plot(xnp,y_lrelu.detach().numpy(),label='Leaky ReLU')\n",
    "plt.plot(xnp,y_silu.detach().numpy(),label='SiLU')\n",
    "plt.xlabel('x',fontsize=15)\n",
    "plt.ylabel('activation function',fontsize=15)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qS1axDDY0Ug"
   },
   "source": [
    "## **In Class Activity - Discuss one of the Activation Functions**\n",
    "\n",
    "**Diagram**\\\n",
    "<img src=\"https://raw.github.com/RodrigoAVargasHdz/CHEM-4PB3/master/Course_Notes/Figures/MLP_diagram.png\"  width=\"400\" height=\"300\">\n",
    "\n",
    "Chose one of the activation functions discussed and use it in your linear model. Which function works the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ANgR_9izZaj3",
    "outputId": "4dff7aea-eb9a-43a0-9234-885411c71864"
   },
   "outputs": [],
   "source": [
    "# Code here\n",
    "# Define a model\n",
    "model = nn.Sequential(\n",
    "nn.Linear(1, 100),\n",
    "nn.SiLU(),\n",
    "nn.Linear(100, 1)\n",
    ")\n",
    "\n",
    "model = training(model,10000)\n",
    "X_grid = torch.linspace(0., 1., 5000).unsqueeze(1)\n",
    "plt.clf()\n",
    "X, y = get_data(25)\n",
    "print(X.shape, y.shape, model(X).shape)\n",
    "plt.scatter(X.detach().numpy(), y.detach().numpy(), label='Batch')\n",
    "plt.plot(X_grid.detach().numpy(), f(X_grid).detach().numpy(),\n",
    "         ls='--', c='k', label=r'$f(x)$')\n",
    "plt.plot(X_grid.detach().numpy(), model(\n",
    "    X_grid).detach().numpy(), c='red', label=r'$NN(x)$')\n",
    "plt.ylabel(r'$f(x)$', fontsize=12)\n",
    "plt.xlabel(r'$x$', fontsize=12)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HLmioAOZ2or"
   },
   "source": [
    "Work in a small groups and discuss the following.\n",
    "1. How many layers we need?\n",
    "2. What is the *best* activation function?\n",
    "\n",
    "\n",
    "# Extra\n",
    "\n",
    "Go to the following [link](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.03345&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) and try to solve all the different tasks!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
