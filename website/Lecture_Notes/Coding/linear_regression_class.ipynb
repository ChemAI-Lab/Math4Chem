{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression \n",
    "\n",
    "This code is to help us visualize the linear regression models. <br>\n",
    "*Due to some time limitations this tutorial is not fully clean and well documented, I apologize for this inconvenience.\n",
    "\n",
    "This tutorial can be deployed in <a target=\"_blank\" href=\"https://colab.research.google.com/github/ChemAI-Lab/Math4Chem/blob/main/website/Lecture_Notes/Coding/linear_regression_class.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the a dummy linear model,\n",
    "$$\n",
    "f(\\mathbf{w},x) = a\\;x + b\n",
    "$$\n",
    "or in a vector notation,\n",
    "$$\n",
    "f(\\mathbf{w},x) = \\begin{bmatrix}\n",
    "b, & a \\end{bmatrix}  \\begin{bmatrix}\n",
    "1 \\\\\n",
    " x \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "As we will see, the parameters of the linear model, $\\mathbf{w}^top=[a,b]$, are *free parameters*. Meaning, we usually use a metric function, commonly named to **loss function**, to search for the **optimal parameters**.<br>\n",
    "\n",
    ". The loss function measures the distance between the predicted values, $f(x,\\mathbf{w})$, and true values, $\\hat{y}$,. <br>\n",
    "\n",
    "For example the square error, \n",
    "$$\n",
    "\\epsilon(x_i,\\mathbf{w}) = \\frac{1}{2}\\left (\\hat{y}_i - f(x_,\\mathbf{w}) \\right )^{2} = \\frac{1}{2}\\left (\\hat{y}_i - (a\\;x_i + b)) \\right )^{2}\n",
    "$$\n",
    "the $\\frac{1}{2}$ factor only rescales the error between the predicted values and the actual values. (hint: $\\frac{d x^2}{d\\;x}$).\n",
    "\n",
    "**Extra**\n",
    "The square error is not the only possible loss function that one can use. <br>\n",
    "For example, the absolute error leads to a family of linear models known as [Least absolute deviations](https://en.wikipedia.org/wiki/Least_absolute_deviations)\n",
    "$$\n",
    "\\epsilon(x,\\mathbf{w}) = \\left | \\hat{y}_i - (a\\;x + b)) \\right |\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when deployed in Colab uncomment this line to install ipyml\n",
    "#!pip install ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.tri as tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(a,b,x):\n",
    "    return a*x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2, 5)\n",
    "y = linear_model(2, 1, x) + np.random.uniform(-3, 3,size=x.shape) # add some noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the values of a and b and see what happens\n",
    "a = -1\n",
    "b = 1\n",
    "\n",
    "x_grid = np.linspace(0, 2, 100)\n",
    "y_grid = linear_model(a, b, x_grid)\n",
    "\n",
    "y_pred = linear_model(a, b, x)\n",
    "error = y_pred - y\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(11,5))\n",
    "fig.suptitle('Error in linear models')\n",
    "\n",
    "# left panel \n",
    "ax1.plot(x_grid, y_grid, c='k')\n",
    "for xi, yi in zip(x, y):\n",
    "    y_pred_i = linear_model(a, b, xi)\n",
    "    error_i = yi - y_pred_i\n",
    "    if error_i > 0:\n",
    "        ax1.vlines(xi, y_pred_i, y_pred_i + np.abs(error_i))\n",
    "    else:\n",
    "        ax1.vlines(xi, y_pred_i - np.abs(error_i), y_pred_i)\n",
    "ax1.text(0.1, .9, f'a = {a}, b = {b}', fontsize=20, transform=ax1.transAxes)\n",
    "ax1.scatter(x, y, s=45)\n",
    "ax1.set_xlim(-0.5, 2.5)\n",
    "ax1.set_ylim(-2, 12)\n",
    "ax1.set_ylabel(r'$a\\;x + b$', fontsize=20)\n",
    "ax1.set_xlabel(r'$x$', fontsize=20)\n",
    "\n",
    "# right panel\n",
    "sum_errors = 0.\n",
    "for xi, yi in zip(x, y):\n",
    "    y_pred_i = linear_model(a, b, xi)\n",
    "    error_i = yi - y_pred_i\n",
    "    sum_errors += error_i**2\n",
    "\n",
    "    ax2.scatter(xi, error_i**2, color='k', s=50)\n",
    "    ax2.scatter(xi, np.abs(error_i), color='r', s=50, marker='s')\n",
    "ax2.text(0.1, 0.8, r'$\\sum_i^n \\epsilon_i^2 = $' + f'{sum_errors:.2f}',\n",
    "         transform=ax2.transAxes, fontsize=16)\n",
    "ax2.set_ylabel(r'Error', fontsize=20)\n",
    "ax2.set_xlabel(r'$x$', fontsize=20)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean squared error\n",
    "From what we can observe, each point give us an error tha estimates how accurate or inaccurate is our model.<br>\n",
    "Therefore to measure the quality of the the model on the entire collection of data points, **we simply average the individual errors**.<br>\n",
    "The Mean Squared Error (MSE) is defined as:\n",
    "\n",
    "$$\n",
    "{\\cal L}(\\mathbf{w}) = \\frac{1}{n} \\sum_{i=i}^{n} \\ell_i(\\mathbf{w}) = \\frac{1}{n} \\sum_{i=i}^{n} \\frac{1}{2} \\left (\\hat{y}_i - \\mathbf{w}^\\top x \\right )^2 = \\frac{1}{n} \\sum_{i=i}^{n} \\frac{1}{2} \\left (\\hat{y}_i - a\\;x + b \\right )^2,\n",
    "$$\n",
    "where $\\mathbf{w}$ is $[a,b]$, the parameters of a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal model\n",
    "\n",
    "Finding the optimal model (also known as training in machine learning), meaning the value of $\\mathbf{w}$, where ${\\cal L}(\\mathbf{w})$ has the lowest value. \n",
    "$$\n",
    "\\mathbf{w}^* = \\argmin_{\\mathbf{w}} {\\cal L}(\\mathbf{w}).\n",
    "$$\n",
    "\n",
    "For this lecture, we will use a grid search approach to search for $\\mathbf{w}^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_total_error(a, b, x,y):\n",
    "    y_pred = linear_model(a, b, x)\n",
    "    error = y_pred - y\n",
    "    error_sqr = error**2\n",
    "    return 0.5 * np.mean(error_sqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_grid = np.linspace(-5,15,100)\n",
    "b_grid = np.arange(-3,4,1)\n",
    "\n",
    "plt.figure(figsize=(11,8))\n",
    "for bi in b_grid:\n",
    "    total_errors = [linear_model_total_error(a, bi, x, y) for a in a_grid]\n",
    "    total_errors = np.asanyarray(total_errors)\n",
    "    i0 = np.argmin(total_errors)\n",
    "    a_best = a_grid[i0]\n",
    "    plt.plot(a_grid, total_errors,label=bi)\n",
    "    plt.scatter(a_best,total_errors[i0],color='k',zorder=2.5)\n",
    "    \n",
    "plt.legend()\n",
    "# plt.ylim(0,10)\n",
    "plt.xlabel('a',fontsize=15)\n",
    "plt.ylabel('Mean Square Error', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_grid = np.linspace(-1, 3, 100)\n",
    "b_grid = np.linspace(-1, 3,  100)\n",
    "a_, b_ = np.meshgrid(a_grid, b_grid)\n",
    "a_b = np.column_stack((a_.flatten(), b_.flatten()))\n",
    "\n",
    "total_error_ = []\n",
    "for abi in a_b:\n",
    "    a,b = abi[0],abi[1]\n",
    "    ei = linear_model_total_error(a, b, x, y)\n",
    "    total_error_.append(ei)\n",
    "total_error_ = np.array(total_error_)\n",
    "i0 = np.argmin(total_error_)\n",
    "\n",
    "ab_best = a_b[i0]\n",
    "lowest_error = total_error_[i0]\n",
    "print(ab_best, lowest_error)\n",
    "total_error_ = total_error_.reshape(a_.shape)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(11, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(ab_best[0], ab_best[1], lowest_error,\n",
    "           zorder=10, color='k', marker='x', s=125)  # + 1E-3 is only for display\n",
    "ax.plot_surface(a_, b_, total_error_, rstride=1, cstride=1,\n",
    "                cmap='viridis', edgecolor='none')\n",
    "\n",
    "ax.set_xlabel('a',fontsize=25)\n",
    "ax.set_ylabel('b', fontsize=25)\n",
    "ax.set_zlabel('RMSE', fontsize=15,rotation=90)\n",
    "# ax.view_init(elev=30, azim=-120)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_grid = np.linspace(-2, 3, 100)\n",
    "b_grid = np.linspace(-2, 3,  100)\n",
    "a_, b_ = np.meshgrid(a_grid, b_grid)\n",
    "a_b = np.column_stack((a_.flatten(), b_.flatten()))\n",
    "\n",
    "total_error_ = []\n",
    "for abi in a_b:\n",
    "    a, b = abi[0], abi[1]\n",
    "    ei = linear_model_total_error(a, b, x, y)\n",
    "    total_error_.append(ei)\n",
    "total_error_ = np.array(total_error_)\n",
    "i0 = np.argmin(total_error_)\n",
    "\n",
    "ab_best = a_b[i0]\n",
    "lowest_error = total_error_[i0]\n",
    "print(ab_best, lowest_error)\n",
    "total_error_ = total_error_.reshape(a_.shape)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "CS = ax.contourf(a_, b_, total_error_,levels=50)\n",
    "ax.scatter(ab_best[0], ab_best[1],\n",
    "           zorder=10, color='w', marker='x', s=105)\n",
    "ax.clabel(CS, inline=True, fontsize=10)\n",
    "ax.set_xlabel('a', fontsize=25)\n",
    "ax.set_ylabel('b', fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = ab_best[0], ab_best[1]\n",
    "x_grid = np.linspace(0, 2, 100)\n",
    "y_grid = linear_model(a, b, x_grid)\n",
    "\n",
    "y_pred = linear_model(a, b, x)\n",
    "error = y_pred - y\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 5))\n",
    "fig.suptitle('Error in linear models')\n",
    "\n",
    "# left panel\n",
    "ax1.plot(x_grid, y_grid, c='k')\n",
    "for xi, yi in zip(x, y):\n",
    "    y_pred_i = linear_model(a, b, xi)\n",
    "    error_i = yi - y_pred_i\n",
    "    if error_i > 0:\n",
    "        ax1.vlines(xi, y_pred_i, y_pred_i + np.abs(error_i))\n",
    "    else:\n",
    "        ax1.vlines(xi, y_pred_i - np.abs(error_i), y_pred_i)\n",
    "ax1.text(0.1, .9, f'a = {a:.2f}, b = {b:.2f}',\n",
    "         fontsize=20, transform=ax1.transAxes)\n",
    "ax1.scatter(x, y, s=45)\n",
    "ax1.set_xlim(-0.5, 2.5)\n",
    "ax1.set_ylim(-2, 12)\n",
    "ax1.set_ylabel(r'$a\\;x + b$', fontsize=20)\n",
    "ax1.set_xlabel(r'$x$,', fontsize=20)\n",
    "\n",
    "# right panel\n",
    "sum_errors = 0.\n",
    "for xi, yi in zip(x, y):\n",
    "    y_pred_i = linear_model(a, b, xi)\n",
    "    error_i = yi - y_pred_i\n",
    "    sum_errors += error_i**2\n",
    "\n",
    "    ax2.scatter(xi, error_i**2, color='k', s=50)\n",
    "ax2.text(0.1, 0.8, r'$\\sum_i^n \\epsilon_i^2 = $' + f'{sum_errors:.2f}',\n",
    "         transform=ax2.transAxes, fontsize=16)\n",
    "ax2.set_ylabel(r'$(\\hat{y}_i - f(x_i))^2$', fontsize=20)\n",
    "ax2.set_xlabel(r'$x$', fontsize=20)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear models\n",
    "\n",
    "We are usually defined linear models as functions, $f(x,\\mathbf{w})= a\\;x + b$. <br>\n",
    "However, they can also be defined in terms of matrix-vector multiplication. <br>\n",
    "Let's assume we have a collection of 10 points and we want to use our linear model for prediction.<br>\n",
    "We can represent each point as an ``new'' vector to account for the *bias term*.\n",
    "\n",
    "Let's build the design matrix for these ($\\mathbf{X}$) ten points, \n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "\\mathbf{x}_{0}^\\top \\\\\n",
    "\\mathbf{x}_{1}^\\top \\\\\n",
    " \\vdots \\\\\n",
    "\\mathbf{x}_{10}^\\top \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & x_{0} \\\\\n",
    "1 & x_{1} \\\\\n",
    " \\vdots \\\\\n",
    "1 & x_{10} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Prediction with linear models\n",
    "$$\n",
    "\\underbrace{\\mathbf{X}}_{(10,2)} \\underbrace{\\mathbf{w}}_{(2,1)} = \\begin{bmatrix}\n",
    "\\mathbf{x}_{0}^\\top \\\\\n",
    "\\mathbf{x}_{1}^\\top \\\\\n",
    " \\vdots \\\\\n",
    "\\mathbf{x}_{10}^\\top \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "b \\\\\n",
    "a \\\\\n",
    "\\end{bmatrix}= \\begin{bmatrix}\n",
    "1 & x_{0} \\\\\n",
    "1 & x_{1} \\\\\n",
    " \\vdots \\\\\n",
    "1 & x_{10} \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "b \\\\\n",
    "a \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "b + a\\;x_{0} \\\\\n",
    "b + a\\;x_{1} \\\\\n",
    " \\vdots \\\\\n",
    "b + a\\;x_{10} \\\\\n",
    "\\end{bmatrix} = \\underbrace{\\begin{bmatrix}\n",
    "fx_{0}) \\\\\n",
    "f(x_{1}) \\\\\n",
    " \\vdots \\\\\n",
    "f(x_{10}) \\\\\n",
    "\\end{bmatrix}}_{(10,1)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the design matrix\n",
    "\n",
    "x_grid = np.linspace(-0,2,10) #change the number of points\n",
    "X = np.column_stack((np.ones_like(x_grid), x_grid))\n",
    "print(X.shape)\n",
    "print(X)\n",
    "\n",
    "# w = ab_best[:,None]\n",
    "w = np.array([[ab_best[1]],[ab_best[0]]])\n",
    "print(w.shape)\n",
    "print(w)\n",
    "\n",
    "# prediction \n",
    "y_pred = X @ w\n",
    "\n",
    "\n",
    "# plot the result\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "ax.plot(x_grid, y_pred, c='k', marker='o', label='model prediction')\n",
    "ax.scatter(x, y,label='data',s=55)\n",
    "ax.set_xlabel(r'$x$', fontsize=20)\n",
    "ax.set_ylabel(r'$f(x)$', fontsize=20)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem3pc3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
