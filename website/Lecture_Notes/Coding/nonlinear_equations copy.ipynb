{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systems of Nonlinear Equations\n",
    "\n",
    "This code is to help us visualize the solution of nonlinear linear equations. <br>\n",
    "\n",
    "This tutorial can be deployed in <a target=\"_blank\" href=\"https://colab.research.google.com/github/ChemAI-Lab/Math4Chem/blob/main/website/Lecture_Notes/Coding/nonlinear_equations.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Linear Equations\n",
    "\n",
    "We will start first with a review of linear regression but solved using an iterative search approach.<br>\n",
    "\n",
    "\n",
    "Here we define the a dummy linear model,\n",
    "$$\n",
    "f(\\mathbf{w},x) = \\begin{bmatrix}\n",
    "b, & a \\end{bmatrix}  \\begin{bmatrix}\n",
    "1 \\\\\n",
    " x \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "As we will see, the parameters of the linear model, $\\mathbf{w}^top=[a,b]$, are *free parameters*. Meaning, we usually use a metric function, commonly named to **loss function**, to search for the **optimal parameters**.<br>\n",
    "\n",
    ". The loss function measures the distance between the predicted values, $f(x,\\mathbf{w})$, and true values, $\\hat{y}$,. <br>\n",
    "\n",
    "## Least Squares\n",
    "As we saw in clasee, the least squares problem can be described using the matrix notation,\n",
    "$$\n",
    "\n",
    " {\\cal L}(\\mathbf{w}) =  \\frac{1}{2n}\\left ( \\mathbf{Y} - \\mathbf{X} \\mathbf{w} \\right )^\\top \\left ( \\mathbf{Y} - \\mathbf{X} \\mathbf{w} \\right )= \\frac{1}{2n} \\left (\\mathbf{Y}^\\top \\mathbf{Y} - 2\\mathbf{Y}^\\top\\mathbf{X} \\mathbf{w}  + \\mathbf{w}^\\top \\mathbf{X}^\\top\\mathbf{X} \\mathbf{w} \\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when deployed in Colab uncomment this line to install ipyml\n",
    "#!pip install ipympl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.tri as tri\n",
    "\n",
    "rng1 = np.random.default_rng(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2, 10)\n",
    "X = np.column_stack((np.ones_like(x), x))\n",
    "y = X@np.array([1, 2]) + rng1.uniform(-0.75, 0.75, size=x.shape)  # add some noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the values of a and b and see what happens\n",
    "a = 5\n",
    "b = 1\n",
    "w = np.array([b,a])\n",
    "\n",
    "x_grid = np.linspace(0, 2, 100)\n",
    "X_grid = np.column_stack((np.ones_like(x_grid), x_grid))\n",
    "y_grid = X_grid@w\n",
    "\n",
    "y_pred = X@w # calculate values of training data\n",
    "error = y_pred - y\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(11,5))\n",
    "fig.suptitle('Error in linear models',fontsize=18)\n",
    "\n",
    "# left panel \n",
    "ax1.plot(x_grid, y_grid, c='k')\n",
    "for xi, yi in zip(x, y):\n",
    "    y_pred_i = np.array([1,xi])@w\n",
    "    error_i = yi - y_pred_i\n",
    "    if error_i > 0:\n",
    "        ax1.vlines(xi, y_pred_i, y_pred_i + np.abs(error_i))\n",
    "    else:\n",
    "        ax1.vlines(xi, y_pred_i - np.abs(error_i), y_pred_i)\n",
    "ax1.text(0.1, .9, f'f(x) = {a} x  + {b}', fontsize=20, transform=ax1.transAxes)\n",
    "ax1.scatter(x, y, s=45)\n",
    "ax1.set_xlim(-0.5, 2.5)\n",
    "ax1.set_ylim(-2, 12)\n",
    "ax1.set_ylabel(r'$y$', fontsize=20)\n",
    "ax1.set_xlabel(r'$x$', fontsize=20)\n",
    "\n",
    "# right panel\n",
    "sum_errors = 0.\n",
    "for i,(xi, yi) in enumerate(zip(x, y)):\n",
    "    y_pred_i = np.array([1, xi])@w\n",
    "    error_i = yi - y_pred_i\n",
    "    sum_errors += error_i**2\n",
    "\n",
    "    if i == x.shape[0]-1:\n",
    "        ax2.scatter(xi, error_i**2, color='k', s=55,label=r'$\\epsilon^2(x_i)$')\n",
    "        ax2.scatter(xi, np.abs(error_i), color='r', s=55, marker='s',label=r'$|\\epsilon(x_i)|$')   \n",
    "    else:\n",
    "        ax2.scatter(xi, error_i**2, color='k', s=55)\n",
    "        ax2.scatter(xi, np.abs(error_i), color='r', s=55, marker='s')\n",
    "    \n",
    "\n",
    "ax2.set_ylabel(r'Error', fontsize=20)\n",
    "ax2.set_xlabel(r'$x$', fontsize=20)\n",
    "\n",
    "ax1.set_ylim(-0.2,12)\n",
    "plt.legend(loc=0, fontsize=18)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then we will proceed to compute the Jacobian of ${\\cal L}(\\mathbf{w})$ with respect to the parameters of the model, $\\mathbf{w}^\\top = [b,a]$,\n",
    "$$\n",
    " \\frac{\\partial }{\\partial \\mathbf{w}}{\\cal L}(\\mathbf{w}) = \\frac{1}{2n} \\frac{\\partial }{\\partial \\mathbf{w}}\\left (\\mathbf{Y}^\\top \\mathbf{Y} - 2\\mathbf{Y}^\\top\\mathbf{X} \\mathbf{w}  + \\mathbf{w}^\\top \\mathbf{X}^\\top\\mathbf{X} \\mathbf{w} \\right )\n",
    "$$\n",
    "which give us three unique terms,\n",
    "$$\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}} \\mathbf{Y}^\\top \\mathbf{Y} = \\mathbf{0} \\\\\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}} \\mathbf{Y}^\\top\\mathbf{X} \\mathbf{w} = \\mathbf{z} = \\mathbf{X}^\\top  \\mathbf{Y}\\\\\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}}\\mathbf{w}^\\top \\mathbf{X}^\\top\\mathbf{X} \\mathbf{w} = 2 \\mathbf{X}^\\top\\mathbf{X}\\mathbf{w}\n",
    "$$\n",
    "\n",
    "If we combine these three equations and we set the Jacobian to zero, we  get,\n",
    "$$\n",
    " \\frac{\\partial }{\\partial \\mathbf{w}}{\\cal L}(\\mathbf{w}) = \\frac{1}{n} \\left (- \\mathbf{X}^\\top  \\mathbf{Y}  + \\mathbf{X}^\\top\\mathbf{X} \\mathbf{w} \\right ),\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient\n",
    "def gradient_w(w0,x,y):\n",
    "    w = - x.T@y \n",
    "    w = w + x.T@x@w0\n",
    "    return w/x.shape[0]\n",
    "\n",
    "def loss_function(w0,x,y):\n",
    "    y_pred = x@w0\n",
    "    loss = 0.5 * np.mean((y_pred - y)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_solver(X, y):\n",
    "    Xt = X.T\n",
    "    A = Xt@X\n",
    "    z = Xt@y\n",
    "    A_inv = np.linalg.inv(A)\n",
    "    w = A_inv@z\n",
    "    return w  # optimal parameters\n",
    "\n",
    "w_opt = linear_model_solver(X,y)\n",
    "e_opt = loss_function(w_opt,X,y)\n",
    "print(w_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "Is a search method, used to find the minimum and maximum of functions using the gradient of a function.<br>\n",
    "The central point in gradient descent is **direction of largest decrease is** $-\\frac{\\partial f}{\\partial x}$ (the negative gradient of a function.)<br>\n",
    "\n",
    "The general equation for gradient descent is,\n",
    "$$\n",
    "\\mathbf{w}_{new} = \\mathbf{w} - \\eta \\nabla_{\\mathbf{w}} {\\cal L}(\\mathbf{w})\n",
    "$$\n",
    "where $\\eta$ is the learning rate, and $\\nabla_{\\mathbf{w}} {\\cal L}(\\mathbf{w})$ is the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_grid = np.linspace(-1, 3, 50)\n",
    "b_grid = np.linspace(-1, 3, 50)\n",
    "a_, b_ = np.meshgrid(a_grid, b_grid)\n",
    "w_ = np.column_stack((b_.flatten(), a_.flatten()))\n",
    "\n",
    "total_error_ = []\n",
    "for wi in w_:\n",
    "    ei = loss_function(wi, X, y)\n",
    "    total_error_.append(ei)\n",
    "total_error_ = np.array(total_error_)\n",
    "total_error_ = total_error_.reshape(a_.shape)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(11, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(b_, a_, total_error_, rstride=1, cstride=1,\n",
    "                cmap='viridis', edgecolor='none', alpha=0.5)\n",
    "\n",
    "eta = 0.1\n",
    "w_random = rng1.normal(-0.5, 0.5, size=(2,))\n",
    "# w_random = np.ones((2,))\n",
    "ax.set_title(r'$\\eta$ = ' + f'{eta:.2f}',fontsize=18)\n",
    "\n",
    "for i in range(10):\n",
    "    e = loss_function(w_random,X,y)\n",
    "    dw = gradient_w(w_random, X, y)\n",
    "    print(i,dw)\n",
    "    w_new = w_random - eta*dw\n",
    "    e_new = loss_function(w_new,X,y)\n",
    "    ax.quiver(w_random[0], w_random[1], e, w_new[0]-w_random[0],\n",
    "            w_new[1]-w_random[1], e_new-e)\n",
    "    ax.scatter(w_random[0], w_random[1], e, marker='x', s=55,color='k')\n",
    "    w_random = w_new\n",
    "\n",
    "ax.scatter(w_opt[0], w_opt[1], e_opt, marker='o', s=55, color='r')\n",
    "ax.set_xlabel('b', fontsize=25)\n",
    "ax.set_ylabel('a', fontsize=25)\n",
    "ax.set_zlabel('RMSE', fontsize=15, rotation=90)\n",
    "# ax.view_init(elev=30, azim=-120)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_grad_trajectory(eta,n_itr):\n",
    "    # w = np.random.normal(-5, 5, size=(2,))\n",
    "    w = np.random.uniform(-1, 2, size=(2,))\n",
    "    # w = np.ones((2,))\n",
    "    w_tr = w[None, :]\n",
    "    for i in range(n_itr):\n",
    "        dw = gradient_w(w, X, y)\n",
    "        e = loss_function(w, X, y)\n",
    "        w = w - eta * dw\n",
    "        w_tr = np.vstack((w_tr, w))\n",
    "    return w_tr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_grid = np.linspace(-1, 3, 50)\n",
    "b_grid = np.linspace(-1, 3, 50)\n",
    "b_, a_ = np.meshgrid(a_grid, b_grid)\n",
    "w_ = np.column_stack((b_.flatten(), a_.flatten()))\n",
    "\n",
    "total_error_ = []\n",
    "for wi in w_:\n",
    "    ei = loss_function(wi, X, y)\n",
    "    total_error_.append(ei)\n",
    "total_error_ = np.array(total_error_)\n",
    "total_error_ = total_error_.reshape(a_.shape)\n",
    "\n",
    "eta = 0.2\n",
    "n_itr = 10\n",
    "\n",
    "plt.figure(figsize=(11, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(b_, a_, total_error_, rstride=1, cstride=1,\n",
    "                cmap='viridis', edgecolor='none',alpha=0.5)\n",
    "\n",
    "# random initialization\n",
    "for i in range(10):\n",
    "    w_tr = get_grad_trajectory(eta,n_itr)\n",
    "    for wi in w_tr:\n",
    "        ax.scatter(wi[0], wi[1], loss_function(wi, X,y),\n",
    "            zorder=10, color='k', marker='x', s=15,linewidths=1.)  # + 1E-3 is only for display\n",
    "\n",
    "ax.scatter(w_opt[0], w_opt[1], e_opt, marker='o', s=55, color='r')\n",
    "ax.set_xlabel('b', fontsize=25)\n",
    "ax.set_ylabel('a', fontsize=25)\n",
    "ax.set_zlabel('RMSE', fontsize=15, rotation=90)\n",
    "# ax.view_init(elev=30, azim=-120)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the values of a and b and see what happens\n",
    "x_grid = np.linspace(0, 2, 100)\n",
    "X_grid = np.column_stack((np.ones_like(x_grid), x_grid))\n",
    "\n",
    "eta = 0.1\n",
    "n_itr = 1000\n",
    "w_tr = get_grad_trajectory(eta,n_itr)\n",
    " \n",
    "fig, (ax1) = plt.subplots(1, 1, figsize=(9, 8))\n",
    "\n",
    "# left panel\n",
    "for i,wi in enumerate(w_tr[::5]):\n",
    "    y_pred = X_grid@wi\n",
    "    ax1.plot(x_grid, y_pred, alpha=0.75)\n",
    "\n",
    "ax1.plot(x_grid, X_grid@w_opt, c='k',label='optimal model')\n",
    "ax1.scatter(x, y, c='k', s=45, label='data',zorder=4)\n",
    "\n",
    "ax1.set_ylabel(r'$y$', fontsize=20)\n",
    "ax1.set_xlabel(r'$x$', fontsize=20)\n",
    "\n",
    "# ax1.set_ylim(-0.2, 12)\n",
    "plt.legend(loc=0, fontsize=18)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for NonLinear Equations\n",
    "\n",
    "Here, we will extend gradient descent for non linear equations. \n",
    "Let's do a simple example of the gradient descent for finding the minima of the following chemical equilibrium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    z = 2*np.log(2*x) - np.log(1-x) - np.log(1-3*x)\n",
    "    return z\n",
    "\n",
    "def df_dx(x):\n",
    "    dz = 4/(2*x)+1/(1-x)+9/(1-3*x)\n",
    "    return dz\n",
    "\n",
    "def error_function(x,k):\n",
    "    return (f(x) - k)**2\n",
    "\n",
    "def gradient_error_function(x,k):\n",
    "    return -2*(k-f(x))*df_dx(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 0.01 # and \n",
    "# x0 = 0.05\n",
    "eta = 1E-5\n",
    "ln_k = np.log(4.34E-3)\n",
    "x0_trj = np.array(x0)\n",
    "for i in range(30):\n",
    "    dx0 = gradient_error_function(x0,ln_k)\n",
    "    e = error_function(x0, ln_k)\n",
    "    print(f'Iteration {i+1}, x={x0:.5f}, f(x)={f(x0):.5f}, e={e:.5f}')\n",
    "    x0 = x0 - eta * dx0\n",
    "    x0_trj = np.append(x0_trj, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = np.linspace(0.01,0.05,1000)\n",
    "error_grid = error_function(x_grid,ln_k)\n",
    "x_opt = x_grid[np.argmin(error_grid)]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.clf()\n",
    "plt.plot(x_grid, error_grid, c='k')\n",
    "plt.scatter(x_opt, error_function(x_opt,ln_k), c='r', marker='o', s=100)\n",
    "plt.scatter(x0_trj, error_function(x0_trj, ln_k), s=105)\n",
    "plt.scatter(x0, error_function(x0, ln_k), c='k', marker='x', s=100)\n",
    "\n",
    "# plt.xscale('log')\n",
    "plt.xlabel('x',fontsize=20)\n",
    "plt.ylabel('Error', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_residuals(x,m0,k):\n",
    "    xa1 = x[0]\n",
    "    xa2 = x[1]\n",
    "    xw = x[2]\n",
    "    ka1 = k[0]\n",
    "    ka2 = k[1]\n",
    "    kw = k[2]\n",
    "    \n",
    "    g1 = ka1*(m0 + xa1) - (xa1 - xa2)*(xa1 + xa2 + xw)\n",
    "    g2 = ka2*(xa1 - xa2) - (xa2)*(xa1 + xa2 + xw)\n",
    "    gw = kw -xw*(xa1 + xa2 + xw)\n",
    "    return np.array([g1,g2,gw])\n",
    "\n",
    "def jacobian_f_r(x,m0,k):\n",
    "    xa1 = x[0]\n",
    "    xa2 = x[1]\n",
    "    xw = x[2]\n",
    "    ka1 = k[0]\n",
    "    ka2 = k[1]\n",
    "    kw = k[2]\n",
    "    \n",
    "    dg1_dxa1 = -ka1 - 2*xa1 - xw\n",
    "    dg1_dxa2 = 2*xa2 + xw\n",
    "    dg1_dxw = -xa1 + xa2\n",
    "    \n",
    "    dg2_dxa1 = ka2 - xa2\n",
    "    dg2_dxa2 = -ka2 + 2*xa2 - xa1 - xw\n",
    "    dg2_dxw = -xa2\n",
    "    \n",
    "    dg3_dxa1 = -xw\n",
    "    dg3_dxa2 = -xw\n",
    "    dg3_dxw = -xa1 - xa2 - 2*xw\n",
    "    \n",
    "    return np.array([[dg1_dxa1, dg1_dxa2, dg1_dxw], \n",
    "                     [dg2_dxa1, dg2_dxa2, dg2_dxw], \n",
    "                     [dg3_dxa1, dg3_dxa2, dg3_dxw]])\n",
    "\n",
    "def error_function(x0, m0, k):\n",
    "    g = f_residuals(x0,m0,k)\n",
    "    return 0.5 * np.dot(g,g)\n",
    "\n",
    "def gradient_error_function(x0, m0, k):\n",
    "    g = f_residuals(x0, m0, k)\n",
    "    jac_g = jacobian_f_r(x0, m0, k)\n",
    "    return jac_g.T @ g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.array([5.6E-2,1.5E-4,1E-14])\n",
    "m0 = 1E-4\n",
    "# x0 = np.array([9.8E-5,4.99E-5,6.67E-11])\n",
    "x0 = np.random.normal(-5,5,size=(3,))*1E-5\n",
    "\n",
    "print(f_residuals(x0, m0, k))\n",
    "print(error_function(x0, m0, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1E1\n",
    "x0_trj = np.array(x0[None,:])\n",
    "error_trj = np.array(error_function(x0, m0, k))\n",
    "dx0_norm_trj = np.array(np.linalg.norm(gradient_error_function(x0, m0,k)))\n",
    "for i in range(2000):\n",
    "    dx0 = gradient_error_function(x0, m0,k)\n",
    "    e = error_function(x0, m0,k)\n",
    "    print(f'Itr {i+1}, e = ', e , 'x = ',x0, '|dx| = ', np.linalg.norm(dx0))\n",
    "    x0 = x0 - eta * dx0\n",
    "    x0_trj = np.vstack((x0_trj, x0))\n",
    "    error_trj = np.append(error_trj, e)\n",
    "    dx0_norm_trj = np.append(dx0_norm_trj, np.linalg.norm(dx0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.clf()\n",
    "plt.plot(np.arange(error_trj.shape[0]),error_trj,label='error')\n",
    "plt.plot(np.arange(error_trj.shape[0]), dx0_norm_trj, label='norm gradient')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Iteration', fontsize=20)\n",
    "plt.ylabel('Error', fontsize=20)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x0_trj)\n",
    "fig = plt.figure(10,10)\n",
    "plt.clf()\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "xs = np.random.rand(100)\n",
    "ys = np.random.rand(100)\n",
    "zs = np.random.rand(100)\n",
    "ax.scatter(x0[0], x0[1], x0[2], c='k', marker='x',s=160)\n",
    "sc = ax.scatter(x0_trj[:, 0], x0_trj[:, 1], x0_trj[:, 2], c=error_trj, cmap='viridis')\n",
    "plt.colorbar(sc)\n",
    "ax.set_xlabel(r'$x_{a1}$', fontsize=18)\n",
    "ax.set_ylabel(r'$x_{a2}$', fontsize=18)\n",
    "ax.set_zlabel(r'$x_{w}$', fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem3pc3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
