{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression \n",
    "\n",
    "This code is to help us visualize the linear regression models. <br>\n",
    "*Due to some time limitations this tutorial is not fully clean and well documented, I apologize for this inconvenience.\n",
    "\n",
    "This tutorial can be deployed in <a target=\"_blank\" href=\"https://colab.research.google.com/github/ChemAI-Lab/Math4Chem/blob/main/website/Lecture_Notes/Coding/linear_regression.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the a dummy linear model,\n",
    "$$\n",
    "f(\\mathbf{w},x) = a\\;x + b\n",
    "$$\n",
    "or in a vector notation,\n",
    "$$\n",
    "f(\\mathbf{w},x) = \\begin{bmatrix}\n",
    "b, & a \\end{bmatrix}  \\begin{bmatrix}\n",
    "1 \\\\\n",
    " x \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "As we will see, the parameters of the linear model, $\\mathbf{w}^top=[a,b]$, are *free parameters*. Meaning, we usually use a metric function, commonly named to **loss function**, to search for the **optimal parameters**.<br>\n",
    "\n",
    ". The loss function measures the distance between the predicted values, $f(x,\\mathbf{w})$, and true values, $\\hat{y}$,. <br>\n",
    "\n",
    "For example the square error, \n",
    "$$\n",
    "\\epsilon(x_i,\\mathbf{w}) = \\frac{1}{2}\\left (\\hat{y}_i - f(x_,\\mathbf{w}) \\right )^{2} = \\frac{1}{2}\\left (\\hat{y}_i - (a\\;x_i + b)) \\right )^{2}\n",
    "$$\n",
    "the $\\frac{1}{2}$ factor only rescales the error between the predicted values and the actual values. (hint: $\\frac{d x^2}{d\\;x}$).\n",
    "\n",
    "**Extra**\n",
    "The square error is not the only possible loss function that one can use. <br>\n",
    "For example, the absolute error leads to a family of linear models known as [Least absolute deviations](https://en.wikipedia.org/wiki/Least_absolute_deviations)\n",
    "$$\n",
    "\\epsilon(x,\\mathbf{w}) = \\left | \\hat{y}_i - (a\\;x + b)) \\right |\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when deployed in Colab uncomment this line to install ipyml\n",
    "#!pip install ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.tri as tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(a,b,x):\n",
    "    return a*x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2, 5)\n",
    "y = linear_model(2, 1, x) + np.random.uniform(-1, 3,size=x.shape) # add some noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the values of a and b and see what happens\n",
    "a = 5\n",
    "b = 1\n",
    "\n",
    "x_grid = np.linspace(0, 2, 100)\n",
    "y_grid = linear_model(a, b, x_grid)\n",
    "\n",
    "y_pred = linear_model(a, b, x)\n",
    "error = y_pred - y\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(11,5))\n",
    "fig.suptitle('Error in linear models',fontsize=18)\n",
    "\n",
    "# left panel \n",
    "ax1.plot(x_grid, y_grid, c='k')\n",
    "for xi, yi in zip(x, y):\n",
    "    y_pred_i = linear_model(a, b, xi)\n",
    "    error_i = yi - y_pred_i\n",
    "    if error_i > 0:\n",
    "        ax1.vlines(xi, y_pred_i, y_pred_i + np.abs(error_i))\n",
    "    else:\n",
    "        ax1.vlines(xi, y_pred_i - np.abs(error_i), y_pred_i)\n",
    "ax1.text(0.1, .9, f'f(x) = {a} x  + {b}', fontsize=20, transform=ax1.transAxes)\n",
    "ax1.scatter(x, y, s=45)\n",
    "ax1.set_xlim(-0.5, 2.5)\n",
    "ax1.set_ylim(-2, 12)\n",
    "ax1.set_ylabel(r'$y$', fontsize=20)\n",
    "ax1.set_xlabel(r'$x$', fontsize=20)\n",
    "\n",
    "# right panel\n",
    "sum_errors = 0.\n",
    "for i,(xi, yi) in enumerate(zip(x, y)):\n",
    "    y_pred_i = linear_model(a, b, xi)\n",
    "    error_i = yi - y_pred_i\n",
    "    sum_errors += error_i**2\n",
    "\n",
    "    if i == x.shape[0]-1:\n",
    "        ax2.scatter(xi, error_i**2, color='k', s=55,label=r'$\\epsilon^2(x_i)$')\n",
    "        ax2.scatter(xi, np.abs(error_i), color='r', s=55, marker='s',label=r'$|\\epsilon(x_i)|$')   \n",
    "    else:\n",
    "        ax2.scatter(xi, error_i**2, color='k', s=55)\n",
    "        ax2.scatter(xi, np.abs(error_i), color='r', s=55, marker='s')\n",
    "    \n",
    "    \n",
    "# ax2.text(0.1, 0.8, r'$\\sum_i^n \\epsilon_i^2 = $' + f'{sum_errors:.2f}',\n",
    "        #  transform=ax2.transAxes, fontsize=16)\n",
    "ax2.set_ylabel(r'Error', fontsize=20)\n",
    "ax2.set_xlabel(r'$x$', fontsize=20)\n",
    "\n",
    "ax1.set_ylim(-0.2,12)\n",
    "plt.legend(loc=0, fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig_linear_model_errors.png',dpi = 260)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean squared error\n",
    "From what we can observe, each point give us an error tha estimates how accurate or inaccurate is our model.<br>\n",
    "Therefore to measure the quality of the the model on the entire collection of data points, **we simply average the individual errors**.<br>\n",
    "The Mean Squared Error (MSE) is defined as:\n",
    "\n",
    "$$\n",
    "{\\cal L}(\\mathbf{w}) = \\frac{1}{n} \\sum_{i=i}^{n} \\ell_i(\\mathbf{w}) = \\frac{1}{n} \\sum_{i=i}^{n} \\frac{1}{2} \\left (\\hat{y}_i - \\mathbf{w}^\\top x \\right )^2 = \\frac{1}{n} \\sum_{i=i}^{n} \\frac{1}{2} \\left (\\hat{y}_i - a\\;x + b \\right )^2,\n",
    "$$\n",
    "where $\\mathbf{w}$ is $[a,b]$, the parameters of a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal model\n",
    "\n",
    "Finding the optimal model (also known as training in machine learning), meaning the value of $\\mathbf{w}$, where ${\\cal L}(\\mathbf{w})$ has the lowest value. \n",
    "$$\n",
    "\\mathbf{w}^* = \\argmin_{\\mathbf{w}} {\\cal L}(\\mathbf{w}).\n",
    "$$\n",
    "\n",
    "For this lecture, we will use a grid search approach to search for $\\mathbf{w}^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_total_error(a, b, x,y):\n",
    "    y_pred = linear_model(a, b, x)\n",
    "    error = y_pred - y\n",
    "    error_sqr = error**2\n",
    "    return 0.5 * np.mean(error_sqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_grid = np.linspace(-5,15,100)\n",
    "b_grid = np.arange(-3,5,1)\n",
    "\n",
    "plt.figure(figsize=(11,8))\n",
    "for bi in b_grid:\n",
    "    total_errors = [linear_model_total_error(a, bi, x, y) for a in a_grid]\n",
    "    total_errors = np.asanyarray(total_errors)\n",
    "    i0 = np.argmin(total_errors)\n",
    "    a_best = a_grid[i0]\n",
    "    plt.scatter(a_grid, total_errors,label=bi)\n",
    "    plt.plot(a_grid, total_errors)\n",
    "    plt.scatter(a_best,total_errors[i0],color='k',zorder=2.5,marker='x',s=75)\n",
    "    \n",
    "plt.legend()\n",
    "plt.ylim(0,4)\n",
    "plt.xlim(-0,4)\n",
    "plt.xlabel('a',fontsize=15)\n",
    "plt.ylabel('Mean Square Error', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_grid = np.linspace(-1, 3, 50)\n",
    "b_grid = np.linspace(-1, 3, 50)\n",
    "a_, b_ = np.meshgrid(a_grid, b_grid)\n",
    "a_b = np.column_stack((a_.flatten(), b_.flatten()))\n",
    "\n",
    "total_error_ = []\n",
    "for abi in a_b:\n",
    "    a,b = abi[0],abi[1]\n",
    "    ei = linear_model_total_error(a, b, x, y)\n",
    "    total_error_.append(ei)\n",
    "total_error_ = np.array(total_error_)\n",
    "i0 = np.argmin(total_error_)\n",
    "\n",
    "ab_best = a_b[i0]\n",
    "lowest_error = total_error_[i0]\n",
    "print(ab_best, lowest_error)\n",
    "total_error_ = total_error_.reshape(a_.shape)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(11, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(ab_best[0], ab_best[1], lowest_error,\n",
    "           zorder=10, color='k', marker='x', s=125)  # + 1E-3 is only for display\n",
    "ax.plot_surface(a_, b_, total_error_, rstride=1, cstride=1,\n",
    "                cmap='viridis', edgecolor='none')\n",
    "\n",
    "ax.set_xlabel('a',fontsize=25)\n",
    "ax.set_ylabel('b', fontsize=25)\n",
    "ax.set_zlabel('RMSE', fontsize=15,rotation=90)\n",
    "# ax.view_init(elev=30, azim=-120)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_grid = np.linspace(-2, 3, 50)\n",
    "b_grid = np.linspace(-2, 3,  50)\n",
    "a_, b_ = np.meshgrid(a_grid, b_grid)\n",
    "a_b = np.column_stack((a_.flatten(), b_.flatten()))\n",
    "\n",
    "total_error_ = []\n",
    "for abi in a_b:\n",
    "    a, b = abi[0], abi[1]\n",
    "    ei = linear_model_total_error(a, b, x, y)\n",
    "    total_error_.append(ei)\n",
    "total_error_ = np.array(total_error_)\n",
    "i0 = np.argmin(total_error_)\n",
    "\n",
    "ab_best = a_b[i0]\n",
    "lowest_error = total_error_[i0]\n",
    "print(ab_best, lowest_error)\n",
    "total_error_ = total_error_.reshape(a_.shape)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "CS = ax.contourf(a_, b_, total_error_,levels=50)\n",
    "ax.scatter(ab_best[0], ab_best[1],\n",
    "           zorder=10, color='w', marker='x', s=225)\n",
    "ax.clabel(CS, inline=True, fontsize=10)\n",
    "ax.set_xlabel('a', fontsize=25)\n",
    "ax.set_ylabel('b', fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = ab_best[0], ab_best[1]\n",
    "x_grid = np.linspace(0, 2, 100)\n",
    "y_grid = linear_model(a, b, x_grid)\n",
    "\n",
    "y_pred = linear_model(a, b, x)\n",
    "error = y_pred - y\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 5))\n",
    "fig.suptitle('Error in linear models')\n",
    "\n",
    "# left panel\n",
    "ax1.plot(x_grid, y_grid, c='k')\n",
    "for xi, yi in zip(x, y):\n",
    "    y_pred_i = linear_model(a, b, xi)\n",
    "    error_i = yi - y_pred_i\n",
    "    if error_i > 0:\n",
    "        ax1.vlines(xi, y_pred_i, y_pred_i + np.abs(error_i))\n",
    "    else:\n",
    "        ax1.vlines(xi, y_pred_i - np.abs(error_i), y_pred_i)\n",
    "ax1.text(0.1, .9, f'a = {a:.3f}, b = {b:.3f}',\n",
    "         fontsize=20, transform=ax1.transAxes)\n",
    "ax1.scatter(x, y, s=45)\n",
    "ax1.set_xlim(-0.5, 2.5)\n",
    "ax1.set_ylim(-2, 12)\n",
    "ax1.set_ylabel(r'$a\\;x + b$', fontsize=20)\n",
    "ax1.set_xlabel(r'$x$,', fontsize=20)\n",
    "\n",
    "# right panel\n",
    "sum_errors = 0.\n",
    "for xi, yi in zip(x, y):\n",
    "    y_pred_i = linear_model(a, b, xi)\n",
    "    error_i = yi - y_pred_i\n",
    "    sum_errors += error_i**2\n",
    "\n",
    "    ax2.scatter(xi, error_i**2, color='k', s=50)\n",
    "ax2.text(0.1, 0.8, r'$\\sum_i^n \\epsilon_i^2 = $' + f'{sum_errors:.4f}',\n",
    "         transform=ax2.transAxes, fontsize=16)\n",
    "ax2.set_ylabel(r'$(\\hat{y}_i - f(x_i))^2$', fontsize=20)\n",
    "ax2.set_xlabel(r'$x$', fontsize=20)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear models\n",
    "\n",
    "We are usually defined linear models as functions, $f(x,\\mathbf{w})= a\\;x + b$. <br>\n",
    "However, they can also be defined in terms of matrix-vector multiplication. <br>\n",
    "Let's assume we have a collection of 10 points and we want to use our linear model for prediction.<br>\n",
    "We can represent each point as an ``new'' vector to account for the *bias term*.\n",
    "\n",
    "Let's build the design matrix for these ($\\mathbf{X}$) ten points, \n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "\\mathbf{x}_{0}^\\top \\\\\n",
    "\\mathbf{x}_{1}^\\top \\\\\n",
    " \\vdots \\\\\n",
    "\\mathbf{x}_{10}^\\top \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & x_{0} \\\\\n",
    "1 & x_{1} \\\\\n",
    " \\vdots \\\\\n",
    "1 & x_{10} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Prediction with linear models\n",
    "$$\n",
    "\\underbrace{\\mathbf{X}}_{(10,2)} \\underbrace{\\mathbf{w}}_{(2,1)} = \\begin{bmatrix}\n",
    "\\mathbf{x}_{0}^\\top \\\\\n",
    "\\mathbf{x}_{1}^\\top \\\\\n",
    " \\vdots \\\\\n",
    "\\mathbf{x}_{10}^\\top \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "b \\\\\n",
    "a \\\\\n",
    "\\end{bmatrix}= \\begin{bmatrix}\n",
    "1 & x_{0} \\\\\n",
    "1 & x_{1} \\\\\n",
    " \\vdots \\\\\n",
    "1 & x_{10} \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "b \\\\\n",
    "a \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "b + a\\;x_{0} \\\\\n",
    "b + a\\;x_{1} \\\\\n",
    " \\vdots \\\\\n",
    "b + a\\;x_{10} \\\\\n",
    "\\end{bmatrix} = \\underbrace{\\begin{bmatrix}\n",
    "fx_{0}) \\\\\n",
    "f(x_{1}) \\\\\n",
    " \\vdots \\\\\n",
    "f(x_{10}) \\\\\n",
    "\\end{bmatrix}}_{(10,1)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the design matrix\n",
    "\n",
    "x_grid = np.linspace(-0,2,100) #change the number of points\n",
    "X = np.column_stack((np.ones_like(x_grid), x_grid))\n",
    "print(X.shape)\n",
    "print(X)\n",
    "\n",
    "# w = ab_best[:,None]\n",
    "w = np.array([[ab_best[1]],[ab_best[0]]])\n",
    "print(w.shape)\n",
    "print(w)\n",
    "\n",
    "# prediction \n",
    "y_pred = X @ w\n",
    "\n",
    "\n",
    "# plot the result\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "ax.scatter(x, y, label='data', s=75)\n",
    "ax.plot(x_grid, y_pred, c='k', marker='o', label='model prediction',markersize=5)\n",
    "\n",
    "ax.set_xlabel(r'$x$', fontsize=20)\n",
    "ax.set_ylabel(r'$f(x)$', fontsize=20)\n",
    "ax.legend(fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares\n",
    "Here we will code the solution of the least squares problem using the matrix notation that we cover in class.\n",
    "First, we will define the mean squared error as,\n",
    "$$\n",
    "\n",
    " {\\cal L}(\\mathbf{w}) = \\frac{1}{2n} \\left (\\mathbf{Y}^\\top \\mathbf{Y} - 2\\mathbf{Y}^\\top\\mathbf{X} \\mathbf{w}  + \\mathbf{w}^\\top \\mathbf{X}^\\top\\mathbf{X} \\mathbf{w} \\right )\n",
    "$$\n",
    "Then we will proceed to compute the Jacobian of ${\\cal L}(\\mathbf{w})$ with respect to the parameters of the model, $\\mathbf{w}^\\top = [b,a]$,\n",
    "$$\n",
    " \\frac{\\partial }{\\partial \\mathbf{w}}{\\cal L}(\\mathbf{w}) = \\frac{1}{2n} \\frac{\\partial }{\\partial \\mathbf{w}}\\left (\\mathbf{Y}^\\top \\mathbf{Y} - 2\\mathbf{Y}^\\top\\mathbf{X} \\mathbf{w}  + \\mathbf{w}^\\top \\mathbf{X}^\\top\\mathbf{X} \\mathbf{w} \\right )\n",
    "$$\n",
    "which give us three unique terms,\n",
    "$$\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}} \\mathbf{Y}^\\top \\mathbf{Y} = \\mathbf{0} \\\\\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}} \\mathbf{Y}^\\top\\mathbf{X} \\mathbf{w} = \\mathbf{z} = \\mathbf{X}^\\top  \\mathbf{Y}\\\\\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}}\\mathbf{w}^\\top \\mathbf{X}^\\top\\mathbf{X} \\mathbf{w} = 2 \\mathbf{X}^\\top\\mathbf{X}\\mathbf{w}\n",
    "$$\n",
    "\n",
    "If we combine these three equations and we set the Jacobian to zero, we  get,\n",
    "$$\n",
    " \\mathbf{0} = \\frac{1}{2n} \\left (\\mathbf{0} - 2\\mathbf{X}^\\top  \\mathbf{Y}  + 2\\mathbf{X}^\\top\\mathbf{X} \\mathbf{w} \\right ),\n",
    "$$\n",
    "where after cleaning it up, we get,\n",
    "$$\n",
    "\\mathbf{X}^\\top\\mathbf{X} \\mathbf{w}  = \\mathbf{X}^\\top  \\mathbf{Y}\n",
    "$$\n",
    "\n",
    "This matrix equation can be solved by inverting $\\mathbf{X}^\\top\\mathbf{X}$, \n",
    "$$\n",
    "\\mathbf{w}  = {(\\mathbf{X}^\\top\\mathbf{X})}^{-1}\\mathbf{X}^\\top  \\mathbf{Y}\n",
    "$$\n",
    "This is the central equation of this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "print('Training data')\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# design matrix\n",
    "X_tr = np.column_stack((np.ones_like(x),x))\n",
    "print('Design matrix')\n",
    "print(X_tr)\n",
    "\n",
    "# matrix A\n",
    "A = X_tr.T@X_tr\n",
    "print('Matrix A')\n",
    "print(A)\n",
    "\n",
    "# vector z\n",
    "z = X_tr.T@y\n",
    "print('Vector z')\n",
    "print(z)\n",
    "\n",
    "\n",
    "# solve for w\n",
    "A_inv = np.linalg.inv(A)\n",
    "w_opt = A_inv@z\n",
    "print('Optimal solution for the Mean Square problem')\n",
    "print(w_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_grid = np.linspace(-2, 3, 25)\n",
    "b_grid = np.linspace(-2, 3,  25)\n",
    "a_, b_ = np.meshgrid(a_grid, b_grid)\n",
    "a_b = np.column_stack((a_.flatten(), b_.flatten()))\n",
    "\n",
    "total_error_ = []\n",
    "for abi in a_b:\n",
    "    a, b = abi[0], abi[1]\n",
    "    ei = linear_model_total_error(a, b, x, y)\n",
    "    total_error_.append(ei)\n",
    "total_error_ = np.array(total_error_)\n",
    "total_error_ = total_error_.reshape(a_.shape)\n",
    "\n",
    "\n",
    "b_opt, a_opt = w_opt[0], w_opt[1]\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "CS = ax.contourf(a_, b_, total_error_, levels=50)\n",
    "ax.scatter(ab_best[0], ab_best[1],\n",
    "           zorder=10, color='w', marker='x', s=225,label='grid search')\n",
    "ax.scatter(a_opt, b_opt,\n",
    "           zorder=10, color='w', marker='v', s=225, label='optimal solution')\n",
    "ax.clabel(CS, inline=True, fontsize=10)\n",
    "ax.set_xlabel('a', fontsize=25)\n",
    "ax.set_ylabel('b', fontsize=25)\n",
    "ax.legend(loc='upper left', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction with these parameters is no different. \n",
    "\n",
    "$$\n",
    "f(\\mathbf{X}_{new}) = \\mathbf{X}_{new}^\\top \\mathbf{w}^* = \\mathbf{X}_{new}^\\top \\underbrace{{(\\mathbf{X}^\\top\\mathbf{X})}^{-1}\\mathbf{X}^\\top  \\mathbf{Y}}_{\\mathbf{w}^*} \\\\\n",
    "$$\n",
    "we do not need to compute ${(\\mathbf{X}^\\top\\mathbf{X})}^{-1}\\mathbf{X}^\\top  \\mathbf{Y}$ every time we do prediction, we only need to solve for it ones and store the vector $\\mathbf{w}^*$.\n",
    "\n",
    "<!-- f(\\mathbf{X}) = \\begin{bmatrix}\n",
    "1 & x_{0} \\\\\n",
    "1 & x_{1} \\\\\n",
    " \\vdots \\\\\n",
    "1 & x_{10} \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "b \\\\\n",
    "a\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "a\\;x_1+b \\\\\n",
    "a\\;x_2+b \\\\\n",
    "\\vdots \\\\\n",
    "a\\;x_n+b \\\\\n",
    "\\end{bmatrix} -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the design matrix\n",
    "\n",
    "x_grid = np.linspace(-0, 2, 100)  # change the number of points\n",
    "X = np.column_stack((np.ones_like(x_grid), x_grid))\n",
    "print(X.shape)\n",
    "print(X)\n",
    "\n",
    "# prediction\n",
    "y_pred = X @ w_opt\n",
    "\n",
    "\n",
    "# plot the result\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "ax.scatter(x, y, label='data', s=75)\n",
    "ax.plot(x_grid, y_pred, c='k', marker='o',\n",
    "        label='model prediction', markersize=5)\n",
    "\n",
    "ax.set_xlabel(r'$x$', fontsize=20)\n",
    "ax.set_ylabel(r'$f(x)$', fontsize=20)\n",
    "ax.legend(fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, a = w_opt[0], w_opt[1]\n",
    "x_grid = np.linspace(0, 2, 100)\n",
    "y_grid = linear_model(a, b, x_grid)\n",
    "\n",
    "# y_pred = linear_model(a, b, x)\n",
    "y_pred = np.column_stack((np.ones_like(x), x)) @ w_opt\n",
    "error = y_pred - y\n",
    "\n",
    "\n",
    "# plotting code\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 5))\n",
    "fig.suptitle('Error in linear models')\n",
    "\n",
    "# left panel\n",
    "ax1.plot(x_grid, y_grid, c='k')\n",
    "for xi, yi in zip(x, y):\n",
    "    y_pred_i = linear_model(a, b, xi)\n",
    "    error_i = yi - y_pred_i\n",
    "    if error_i > 0:\n",
    "        ax1.vlines(xi, y_pred_i, y_pred_i + np.abs(error_i))\n",
    "    else:\n",
    "        ax1.vlines(xi, y_pred_i - np.abs(error_i), y_pred_i)\n",
    "ax1.text(0.1, .9, f'a = {a:.3f}, b = {b:.3f}',\n",
    "         fontsize=20, transform=ax1.transAxes)\n",
    "ax1.scatter(x, y, s=45)\n",
    "ax1.set_xlim(-0.5, 2.5)\n",
    "ax1.set_ylim(-2, 12)\n",
    "ax1.set_ylabel(r'$a\\;x + b$', fontsize=20)\n",
    "ax1.set_xlabel(r'$x$,', fontsize=20)\n",
    "\n",
    "# right panel\n",
    "sum_errors = 0.\n",
    "for xi, yi in zip(x, y):\n",
    "    y_pred_i = linear_model(a, b, xi)\n",
    "    error_i = yi - y_pred_i\n",
    "    sum_errors += error_i**2\n",
    "\n",
    "    ax2.scatter(xi, error_i**2, color='k', s=50)\n",
    "ax2.text(0.1, 0.8, r'$\\sum_i^n \\epsilon_i^2 = $' + f'{sum_errors:.4f}',\n",
    "         transform=ax2.transAxes, fontsize=16)\n",
    "ax2.set_ylabel(r'$(\\hat{y}_i - f(x_i))^2$', fontsize=20)\n",
    "ax2.set_xlabel(r'$x$', fontsize=20)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem3pc3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
