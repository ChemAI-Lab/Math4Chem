{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with a Polynomial expansion\n",
    "\n",
    "This code is to help us visualize the linear models for polynomials. <br>\n",
    "\n",
    "This tutorial can be deployed in \n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChemAI-Lab/Math4Chem/blob/main/website/Lecture_Notes/Notes/Coding/polynomial_regression_overfitting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.tri as tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random data over f(x) = sin(x) + x - 1\n",
    "def get_data(N, bool_biased=True):\n",
    "    # This creates an array x of N linearly spaced values between -1 and 1.\n",
    "    # bool_biased: if True, adds a bias term (column of ones) to the input data X\n",
    "    \n",
    "    x = np.linspace(-1., 1., N) + np.random.uniform(low=-.1, high=.1, size=N)\n",
    "    y = 1.2*np.sin(2*x) + x - 1.\n",
    "    # Adds random noise to each y value.\n",
    "    y = y + np.random.uniform(low=-.35, high=.35, size=x.shape)\n",
    "    if bool_biased:\n",
    "        X = np.column_stack((np.ones_like(x), x))\n",
    "    else:\n",
    "        X = x[:, None]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data(15, bool_biased=True)\n",
    "\n",
    "plt.plot(X[:,1],y,'o', label='Data')\n",
    "plt.xlabel('x',fontsize=16)\n",
    "plt.ylabel('f(x)', fontsize=16)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Linear Regression\n",
    "**Mean Square Error**\n",
    "$$\n",
    "{\\cal L}(\\mathbf{w}) = \\frac{1}{2n} \\left (\\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right)^\\top \\left (\\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right)\n",
    "$$\n",
    "\n",
    "**Gradient of a function equal to zero**  \n",
    "$$\n",
    "    \\nabla {\\cal L}(\\mathbf{w}) \\Big\\rvert_{\\mathbf{w}^{*}} = \\frac{1}{2n} \\nabla_{\\mathbf{w}} \\left [ \\left (\\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right)^\\top \\left (\\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right) \\right ]= 0\n",
    "$$\n",
    "\n",
    "To solve for $\\mathbf{w}^*$, let's expand $ \\left (\\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right)^\\top \\left (\\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right)$,\n",
    "\n",
    "$$\n",
    "    \\left (\\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right)^\\top \\left (\\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right) = \\mathbf{y}^\\top \\mathbf{y}  - \\mathbf{y}^\\top \\mathbf{X}\\mathbf{w} -  \\mathbf{w}^\\top\\mathbf{X}^\\top\\mathbf{y} +   \\mathbf{w}^\\top\\mathbf{X}^\\top \\mathbf{X}\\mathbf{w}\n",
    "$$\n",
    "$$\n",
    "    \\nabla_{\\mathbf{w}} {\\cal L}(\\mathbf{w}) = \\frac{1}{2n}\\left(  -2 \\mathbf{X}^\\top\\mathbf{y} + 2\\mathbf{X}^\\top\\mathbf{X}\\mathbf{w} \\right) = 0\n",
    "$$\n",
    "\n",
    "**Optimal parameters**\n",
    "\n",
    "$$\n",
    " \\mathbf{w}^* = \\left ( \\mathbf{X}^\\top \\mathbf{X} \\right ) ^{-1} \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "**Old Notes** <br>\n",
    " Least Square:  [![Download PDF](https://img.shields.io/badge/Download_PDF-Click_Here-blue.svg)](https://github.com/ChemAI-Lab/Math4Chem/raw/main/website/Lecture_Notes/Notes/Linear_Regression.pdf)  <br>\n",
    " \n",
    "**Extra:**\n",
    "1. Homework, Proof the above equations.\n",
    "2. [Equations from Sections 2.4.1 and 2.4.2](https://www2.imm.dtu.dk/pubdb/edoc/imm3274.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trining a linear model\n",
    "def linear_model_solver(X, y):\n",
    "    Xt = X.T # transpose of X\n",
    "    A = Xt@X # A = X^T X\n",
    "    z = Xt@y # z = X^T y\n",
    "    A_inv = np.linalg.inv(A) # inverse of A\n",
    "    w = A_inv@z # w = A^-1 z\n",
    "    return w  # optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficient of determination\n",
    "$R^2$ is a statistical measure of how well the regression line fits the data.\n",
    "1. $R^2= 1$, The model perfectly predicts the dependent variable. All data points fall on the regression line. \n",
    "2. $R^2= 0$, The model is no better than just using the mean of the dependent variable to predict values. The independent variable(s) do not explain any of the variance. \n",
    "3. $R^2=0.80$ means that \\(80\\%\\) of the variability in the dependent variable is explained by the independent variable(s) in the model. The remaining \\(20\\%\\) is unexplained.Â \n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{SS_{res}}{SS_{tot}},\n",
    "$$\n",
    "where \n",
    "$$\n",
    "SS_{res} = \\sum_i^n (\\underbrace{\\hat{y}_i}_{\\text{true value}} - \\underbrace{(\\mathbf{w}^\\top \\mathbf{x}_i)}_{f(x_i)})^2\n",
    "$$\n",
    "$$\n",
    "SS_{tot} =  \\sum_i^n \\left(\\hat{y}_i - \\bar{y} \\right)^2, \\quad \\quad \\text{where} \\quad \\quad \\bar{y} = \\frac{1}{n} \\sum_i^n y_i.\n",
    "$$\n",
    "\n",
    "**Homework**<br>\n",
    "Code $R^2$ as a function of $X,y$ and $\\mathbf{w} = [a,b]$.\n",
    "\n",
    "```Python\n",
    "def r2_function(X,y_true,w):\n",
    "    n = X.shape[0]\n",
    "    y_pred =  # prediction\n",
    "\n",
    "    y_bar = \n",
    "    SS_tot = \n",
    "    SS_res = \n",
    "\n",
    "    r2_value = \n",
    "    return r2_value\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the optimal parameters\n",
    "w_opt = linear_model_solver(X, y)\n",
    "\n",
    "y_data_pred = X@w_opt # prediction\n",
    "r2_value = r2_score(y,y_data_pred)\n",
    "print(f'R-squared score: {r2_value:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot our model\n",
    "X_grid = np.linspace(-1.1,1.1,100)\n",
    "X_grid = np.column_stack((np.ones_like(X_grid),X_grid) )\n",
    "y_pred = X_grid@w_opt\n",
    "\n",
    "# plot the data\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "ax.text(0.05,0.8,r'$R^2 = $' + f'{r2_value:0.3f}',transform=ax.transAxes,fontsize=20)\n",
    "ax.scatter(X[:, 1], y, label='data', s=75)\n",
    "ax.plot(X_grid[:, 1], y_pred, c='k',label='Linear model', markersize=5)\n",
    "ax.set_xlabel('x',fontsize=18)\n",
    "ax.set_ylabel('f(x)', fontsize=18)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's revise polynomials:\n",
    "\n",
    "How many terms if we have a second-order polynomial and $d=3$?\n",
    "$$\n",
    "\\begin{align}\n",
    "(1+x_1+x_2+x_3)^3 &= (1+x_1+x_2+x_3)(1+x_1+x_2+x_3)^2 \\\\\n",
    "&= 1+3x_1+3x^2_1+x^3_1+3x_2+6x_1x_2+3x^2_1x_2 \\\\ \n",
    "& +3x_2^2+3x_1x_2^2+x_2^3 +3x_3+6x_1x_3+3x_1^2x_3 \\\\\n",
    "& +6x_2x_3+6x_1x_2x_3+3x_2^2x_3+3x_3^2 \\\\\n",
    "& +3x_1x_3^2+3x_2x_3^2+x_3^3\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This is simply a new representation of $x$\n",
    "$$\n",
    "\\phi(\\mathbf{x}) = [1, x_1, x_2, x_3, \\cdots, x_i x_j, \\cdots, x_i^{ m} x_j^{p}, \\cdots, x_i^{ m} x_j^{p}x_{\\ell}^{r}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear models on basis-set expansion**\n",
    "\n",
    "$$\n",
    "    f(\\mathbf{x},\\mathbf{w}) = \\sum_{i=0}^d w_i \\phi(\\mathbf{x}) = \\mathbf{w}^\\top \\phi(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "\n",
    "* **Loss function**,\n",
    "$$\n",
    "    \\begin{align}\n",
    "    {\\cal L}(\\mathbf{w}) &= \\frac{1}{2n}\\sum_i^N (y_i - f(\\mathbf{x}_i,\\mathbf{w}))^2 = \\frac{1}{2}\\sum_i^N (y_i - \\mathbf{w}^\\top \\phi(\\mathbf{x}_i))^2 \\\\\n",
    "    &= \\frac{1}{2n} \\left (\\mathbf{y} - \\Phi(\\mathbf{X})\\mathbf{w} \\right)^\\top \\left (\\mathbf{y} -  \\Phi(\\mathbf{X})\\mathbf{w} \\right)\n",
    "    \\end{align}\n",
    "$$\n",
    "Homework, proof the above equations.\n",
    "\n",
    "\n",
    "1. What is $\\Phi(\\mathbf{x})$?\n",
    "2. What is the form of the **optimal** parameters $\\mathbf{w}^*$?\n",
    "\n",
    "\n",
    "<!-- **What is $\\Phi(\\mathbf{x})$?**<br>\n",
    "Let's assume a third order polynomial,\n",
    "$$\n",
    "\\phi(x) = [x^0,x^1,x^2,x^3] = \\underbrace{[1,x,x^2,x^3]}_{p\\,\\, \\text{elemets}}\n",
    "$$\n",
    "If we consider 3 points, \n",
    "$$\n",
    "\\Phi(\\mathbf{X}) = \\underbrace{\\begin{pmatrix}\n",
    "\\phi(x_1)^\\top  \\\\\n",
    "\\vdots \\\\\n",
    "\\phi(x_N)^\\top \\\\\n",
    "\\end{pmatrix}}_{(N,p)} = \\begin{pmatrix}\n",
    "1 & x_1^1 & x^2_1 & x^3_1 \\\\\n",
    "1 & x_2^1 & x^2_2 & x^3_2 \\\\\n",
    "\\vdots & & \\cdots & \\vdots \\\\\n",
    "1 & x_N^1 & x^2_N & x^3_N \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**Derive the equations for $\\mathbf{w}^*$**:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\mathbf{w}^*}_{(p,1)} = \\left ( \\Phi(\\mathbf{X})^\\top \\Phi(\\mathbf{X}) \\right ) ^{-1} \\Phi(\\mathbf{X})^\\top \\mathbf{y}\n",
    "\n",
    "$$ -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of coding how to create the polynomial representation, we will use Sklearn. <br>\n",
    "Read the documentation: [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(X, p):\n",
    "    # transform the input data to polynomial features up to degree p\n",
    "    poly = PolynomialFeatures(p)\n",
    "    Phi = poly.fit_transform(X)\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.ones((1,1))\n",
    "x = np.random.normal(-1.,0.1,size=(1,1))\n",
    "print(f'Original x \\n{x}')\n",
    "for p in range(1,4):\n",
    "    x_phi = polynomial_features(x,p)\n",
    "    print(f'For degree {p}, the polynomial features are:\\n{x_phi}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the training data to polynomial features\n",
    "p = 3\n",
    "\n",
    "X = X[:,-1:] # select the last column only (without bias term)\n",
    "X_phi = polynomial_features(X, p)\n",
    "print(X.shape, X_phi.shape)\n",
    "w_phi_opt = linear_model_solver(X_phi, y)\n",
    "print(f'Optimal parameters\\n{w_phi_opt}')\n",
    "\n",
    "y_data_phi_pred = X_phi@w_phi_opt\n",
    "r2_score_poly = r2_score(y, y_data_phi_pred)\n",
    "print(f'R2 polynomial model: {r2_score_poly:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_grid = np.linspace(-1.25,1.25,100)\n",
    "X_grid = X_grid[:,None]\n",
    "\n",
    "X_grid_phi = polynomial_features(X_grid, p)\n",
    "y_pred_phi = X_grid_phi@w_phi_opt\n",
    "\n",
    "# plot the data and the polynomial model\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "ax.scatter(X[:, 0], y, label='data', s=75)\n",
    "ax.plot(X_grid[:, 0], y_pred, c='green', ls = '--',\n",
    "        label='linear model ' + r'$R^2 = $' +\n",
    "        f'{r2_value:0.3f}', markersize=5)\n",
    "ax.plot(X_grid[:, 0], y_pred_phi, c='k',\n",
    "        label=f'polynomial model (p={p}), R2 = {r2_score_poly:.3f}', markersize=5)\n",
    "ax.set_xlabel('x',fontsize=18)\n",
    "ax.set_ylabel('f(x)', fontsize=18)\n",
    "plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 as a function of the polynomial degree\n",
    "p = 4\n",
    "X = X[:, -1:]  # select the last column\n",
    "\n",
    "X_grid = np.linspace(-1.25, 1.25, 200)\n",
    "X_grid = X_grid[:, None]\n",
    "X_grid_w_bias = np.column_stack((np.ones_like(X_grid), X_grid))\n",
    "y_pred = X_grid_w_bias @ w_opt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "ax.scatter(X[:, 0], y, label='data', s=105,zorder=4)\n",
    "ax.plot(X_grid[:, 0], y_pred, c='red', ls='--',\n",
    "        label='p=1' + r'$R^2 = $' +\n",
    "        f'{r2_value:0.3f}', markersize=5)\n",
    "    \n",
    "for p in range(2,11,2):# loop over polynomial degrees\n",
    "    X_phi = polynomial_features(X, p)\n",
    "    w_phi_opt = linear_model_solver(X_phi, y) # we can reuse our function\n",
    "    # print(f'Optimal parameters\\n{w_phi_opt}')\n",
    "\n",
    "    y_data_phi_pred = X_phi@w_phi_opt\n",
    "    r2_score_poly = r2_score(y, y_data_phi_pred)\n",
    "    print(f'degree: {p}, R2= {r2_score_poly:.4f}')\n",
    "\n",
    "    X_grid_phi = polynomial_features(X_grid, p)\n",
    "    y_pred_phi = X_grid_phi@w_phi_opt\n",
    "# plot the data and the polynomial model\n",
    "\n",
    "    ax.plot(X_grid[:, 0], y_pred_phi,\n",
    "            label=f'p={p} ' +  r' $R^2 = $' +\n",
    "            f'{r2_score_poly:0.4f}',  markersize=5, alpha=0.95*p/15)\n",
    "ax.set_xlabel('x', fontsize=18)\n",
    "ax.set_ylabel('f(x)', fontsize=18)\n",
    "ax.set_ylim(-5,3)\n",
    "plt.legend(fontsize=10,loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenge of Overfitting\n",
    "\n",
    "- **Risk in Flexibility**:  Higher-degree polynomials might fit the training data too precisely, capturing noise rather than the underlying trend, a phenomenon known as overfitting.\n",
    "- **Consequences**: An overfitted model performs poorly on new, unseen data, rendering it less effective for predictive purposes.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    ">#### **Learn More:**\n",
    "1. **What is Overfitting**: [Educational Resource](https://www.v7labs.com/blog/overfitting)\n",
    "2. **Regularization in Machine Learning (with Code Examples)**: [Example Application](https://www.dataquest.io/blog/regularization-in-machine-learning/)\n",
    "3. **Polynomial Regression**: [Educational Resource](https://medium.com/analytics-vidhya/polynomial-regression-%EF%B8%8F-e0e20bfbe9d5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem3pc3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
