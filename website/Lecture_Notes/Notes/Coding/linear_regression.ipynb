{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression \n",
    "\n",
    "This code is to help us visualize, \n",
    "1. linear models\n",
    "2. least square problem\n",
    "\n",
    "This tutorial can be deployed in \n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChemAI-Lab/Math4Chem/blob/main/website/Lecture_Notes/Notes/Coding/linear_regression.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model\n",
    "Let's define a linear model,\n",
    "$$\n",
    "f(\\mathbf{w},x) = a\\;x + b\n",
    "$$\n",
    "or in a vector notation,\n",
    "$$\n",
    "f(\\mathbf{w},x) = \\begin{bmatrix}\n",
    "b, & a \\end{bmatrix}  \\begin{bmatrix}\n",
    "1 \\\\\n",
    " x \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "As we will see, the parameters of the linear model, $\\mathbf{w}^\\top=[m,b]$, are *free parameters*. <br>\n",
    "Meaning, we usually use a metric function, commonly named to **loss function**, to search for the **optimal parameters**.<br>\n",
    "\n",
    "The loss function measures the distance between the predicted values, $f(x,\\mathbf{w})$, and true values, $\\hat{y}$,. <br>\n",
    "\n",
    "For example the square error, \n",
    "$$\n",
    "\\epsilon(x_i,\\mathbf{w}) = \\frac{1}{2}\\left (\\hat{y}_i - f(x_,\\mathbf{w}) \\right )^{2} = \\frac{1}{2}\\left (\\hat{y}_i - (m\\;x_i + b)) \\right )^{2}\n",
    "$$\n",
    "the $\\frac{1}{2}$ factor only rescales the error between the predicted values and the actual values. (hint: $\\frac{d x^2}{d\\;x}$).\n",
    "\n",
    "**Extra**:<br>\n",
    "The square error is not the only possible loss function that one can use. <br>\n",
    "For example, the absolute error,\n",
    "$$\n",
    "\\epsilon(x,\\mathbf{w}) = \\left | \\hat{y}_i - (m\\;x + b)) \\right |\n",
    "$$\n",
    "leads to a family of linear models known as [Least absolute deviations](https://en.wikipedia.org/wiki/Least_absolute_deviations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when deployed in Colab uncomment this line to install ipyml\n",
    "#!pip install ipympl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.tri as tri\n",
    "from ipywidgets import interact, FloatSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(m,b,x):\n",
    "    # code here\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate som \"True\" data\n",
    "x = np.linspace(0, 2, 5)\n",
    "m = 2\n",
    "b = 1\n",
    "y = linear_model(m, b, x)\n",
    "y = y + np.random.uniform(-1, 2,size=x.shape) # add some noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the values of a and b and see what happens\n",
    "m = 5 # slope\n",
    "b = 1 # intercept\n",
    "\n",
    "x_grid = np.linspace(0, 2, 100)\n",
    "y_grid = linear_model(m, b, x_grid)\n",
    "\n",
    "y_pred = linear_model(m, b, x)\n",
    "error = y_pred - y\n",
    "\n",
    "\n",
    "# plot the results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(11,5))\n",
    "fig.suptitle('Error in linear models',fontsize=18)\n",
    "\n",
    "# left panel \n",
    "ax1.plot(x_grid, y_grid, c='k') # model prediction\n",
    "for xi, yi in zip(x, y):\n",
    "    y_pred_i = linear_model(m, b, xi)\n",
    "    error_i = yi - y_pred_i # individual error\n",
    "    if error_i > 0:\n",
    "        ax1.vlines(xi, y_pred_i, y_pred_i + np.abs(error_i)) # up vertical line for error\n",
    "    else:\n",
    "        ax1.vlines(xi, y_pred_i - np.abs(error_i), y_pred_i) # down vertical line for error\n",
    "ax1.text(0.1, .9, f'f(x) = {m} x  + {b}', fontsize=20, transform=ax1.transAxes)\n",
    "ax1.scatter(x, y, s=45) # data points\n",
    "ax1.set_xlim(-0.5, 2.5)\n",
    "ax1.set_ylabel(r'$y$', fontsize=20)\n",
    "ax1.set_xlabel(r'$x$', fontsize=20)\n",
    "\n",
    "# right panel\n",
    "error_ = []\n",
    "for xi, yi in zip(x, y):\n",
    "    y_pred_i = linear_model(m, b, xi)\n",
    "    error_i = yi - y_pred_i # individual error\n",
    "    error_.append(error_i)\n",
    "error_ = np.array(error_)\n",
    "ax2.scatter(x, error_**2, color='k', s=55,label=r'$\\epsilon^2(x_i)$')\n",
    "ax2.scatter(x, np.abs(error_), color='r', s=55, marker='s',label=r'$|\\epsilon(x_i)|$')   \n",
    "\n",
    "\n",
    "ax2.set_ylabel(r'Error', fontsize=20)\n",
    "ax2.set_xlabel(r'$x$', fontsize=20)\n",
    "\n",
    "plt.legend(loc=0, fontsize=18)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Minimal, Colab-friendly ipywidgets demo for linear models: y = m x + b ---\n",
    "\n",
    "# Dense grid for the model line\n",
    "x_grid = np.linspace(0, 2, 100)\n",
    "\n",
    "# ---- Plot function (like your draw(n=...)) ----\n",
    "def draw(b=0.0, m=0.0):\n",
    "    y_grid = linear_model(m, b, x_grid)\n",
    "    y_pred = linear_model(m, b, x)\n",
    "\n",
    "    fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # left panel \n",
    "    ax1.plot(x_grid, y_grid, c='k')\n",
    "    for xi, yi in zip(x, y):\n",
    "        y_pred_i = linear_model(m, b, xi)\n",
    "        error_i = yi - y_pred_i\n",
    "        if error_i > 0:\n",
    "            ax1.vlines(xi, y_pred_i, y_pred_i + np.abs(error_i))\n",
    "        else:\n",
    "            ax1.vlines(xi, y_pred_i - np.abs(error_i), y_pred_i)\n",
    "    ax1.text(0.05, .9, f'f(x) = {m:.2f} x  + {b:.2f}',\n",
    "             fontsize=15, transform=ax1.transAxes)\n",
    "    ax1.scatter(x, y, s=45)\n",
    "    ax1.set_xlim(-0.5, 2.5)\n",
    "    ax1.set_ylabel(r'$y$', fontsize=20)\n",
    "    ax1.set_xlabel(r'$x$', fontsize=20)\n",
    "\n",
    "    # right panel\n",
    "    error_ = []\n",
    "    for xi, yi in zip(x, y):\n",
    "        y_pred_i = linear_model(m, b, xi)\n",
    "        error_i = yi - y_pred_i # individual error\n",
    "        error_.append(error_i)\n",
    "    error_ = np.array(error_)\n",
    "    ax2.scatter(x, error_**2, color='k', s=55,label=r'$\\epsilon^2(x_i)$')\n",
    "    # ax2.scatter(x, np.abs(error_), color='r', s=55, marker='s',label=r'$|\\epsilon(x_i)|$')  \n",
    "\n",
    "    ax2.set_ylabel(r'Error', fontsize=20)\n",
    "    ax2.set_xlabel(r'$x$', fontsize=20)\n",
    "    # ax2.set_ylim(-0.5, 15.) \n",
    "    ax2.set_xlim(-0.5, 2.5)\n",
    "    plt.legend(loc=0, fontsize=12)\n",
    "\n",
    "# ---- Widgets: only m and b ----\n",
    "interact(\n",
    "    draw,\n",
    "    b=FloatSlider(value=1.0, min=-2.5, max=2.5, step=0.01, description=\"b\"),\n",
    "    m=FloatSlider(value=2.0, min=-2.5, max=3.5, step=0.01, description=\"m\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error\n",
    "To measure the quality of the the model on the entire collection of data points, **we simply average the individual errors**.<br>\n",
    "The Mean Squared Error (**MSE**) is defined as:\n",
    "\n",
    "$$\n",
    "{\\cal L}(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=i}^{N} \\ell_i(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=i}^{N}  \\underbrace{\\frac{1}{2} \\left (\\hat{y}_i - \\mathbf{w}^\\top x_i \\right )^2}_{\\epsilon(x_i,\\mathbf{w})} = \\frac{1}{N} \\sum_{i=i}^{N} \\frac{1}{2} \\left (\\hat{y}_i - m\\;x_i + b \\right )^2,\n",
    "$$\n",
    "where $\\mathbf{w}$ is $[m,b]$, the parameters of a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal model\n",
    "\n",
    "Finding the optimal model (also known as training a model), means finding the value of $\\mathbf{w}$, where ${\\cal L}(\\mathbf{w})$ has the lowest value. \n",
    "$$\n",
    "\\mathbf{w}^* = \\arg\\min_{\\mathbf{w}} {\\cal L}(\\mathbf{w}).\n",
    "$$\n",
    "\n",
    "First, we will use a **grid search** approach to search for $\\mathbf{w}^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_mse(m, b, x, y):\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a grid over $m$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_grid = np.linspace(-5,15,150) # slope\n",
    "b_grid = np.arange(-1,5,1) # y-intercept\n",
    "\n",
    "plt.figure(figsize=(11,8))\n",
    "for bi in b_grid: # loop ver the y-intercept\n",
    "    total_errors = [linear_model_mse(mi, bi, x, y) for mi in m_grid] # loop over the slope\n",
    "    total_errors = np.asanyarray(total_errors) # list to numpy array\n",
    "    i0 = np.argmin(total_errors) # lowest error\n",
    "    m_best = m_grid[i0] # best slope for a given b\n",
    "    \n",
    "    # plot\n",
    "    plt.scatter(m_grid, total_errors,label= f'b = {bi}')\n",
    "    plt.plot(m_grid, total_errors)\n",
    "    plt.scatter(m_best,total_errors[i0],color='k',zorder=2.5,marker='x',s=105)\n",
    "    \n",
    "plt.legend()\n",
    "plt.ylim(-0.1,4)\n",
    "plt.xlim(-1,5)\n",
    "plt.xlabel('m',fontsize=15)\n",
    "plt.ylabel('Mean Square Error', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python one can \"flatten\" a for loop \n",
    "```python\n",
    "total_errors = [linear_model_mse(mi, b, x, y) for mi in m_grid] # returns a list\n",
    "# total_errors is a list\n",
    "```\n",
    "\n",
    "is the same as, \n",
    "```python\n",
    "error = []\n",
    "for mi in m_grid:\n",
    "    error_i = linear_model_mse(mi, b, x, y) \n",
    "    error.append(error_i)\n",
    "# variable error is a list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a **denser grid** and plot ${\\cal L}$ as a function of $m$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_grid = np.linspace(-0.5, 4, 50)\n",
    "b_grid = np.linspace(-0.5, 4, 50)\n",
    "m_, b_ = np.meshgrid(m_grid, b_grid)\n",
    "m_b = np.column_stack((m_.flatten(), b_.flatten()))\n",
    "\n",
    "# compute total error for each (m,b) pair\n",
    "total_error_ = []\n",
    "for mbi in m_b:\n",
    "    m,b = mbi[0],mbi[1]\n",
    "    ei = linear_model_mse(m, b, x, y)\n",
    "    total_error_.append(ei)\n",
    "total_error_ = np.array(total_error_)\n",
    "\n",
    "# find the best parameters\n",
    "i0 = np.argmin(total_error_)\n",
    "mb_best = m_b[i0]\n",
    "lowest_error = total_error_[i0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b = mb_best[0], mb_best[1]  # optimal parameters\n",
    "print(\"Best parameters (m,b) and lowest error:\")\n",
    "print(f\"m* = {m:3f}\")\n",
    "print(f\"b* = {b:3f}\")\n",
    "print(f\"MSE = {lowest_error:3f}\")\n",
    "print(10*\"--\")\n",
    "\n",
    "draw(b, m) # plot with optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the MSE as a function of $m$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot MSE surface\n",
    "plt.figure(figsize=(11, 8))\n",
    "total_error_ = total_error_.reshape(m_.shape)\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(mb_best[0], mb_best[1], lowest_error,\n",
    "           zorder=100, color='k', marker='o', s=155, label=\"best\")\n",
    "ax.plot_surface(m_, b_, total_error_, rstride=1, cstride=1,\n",
    "                cmap='RdBu_r', edgecolor='none')\n",
    "\n",
    "ax.set_xlabel('m', fontsize=25)\n",
    "ax.set_ylabel('b', fontsize=25)\n",
    "ax.set_zlabel(r'${\\cal L}(m,b)$', fontsize=20, rotation=90)\n",
    "plt.legend()\n",
    "# ax.view_init(elev=30, azim=-120)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear models, matrix-vector notation\n",
    "\n",
    "We are usually defined linear models as functions, $f(x,\\mathbf{w})= m\\;x + b$. <br>\n",
    "However, they can also be defined in terms of matrix-vector multiplication. <br>\n",
    "Let's assume we have a collection of 10 points and we want to use our linear model for prediction.<br>\n",
    "We can represent each point as an ``new'' vector to account for the *bias term*.\n",
    "\n",
    "Let's build the design matrix for these ($\\mathbf{X}$) ten points, \n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "\\mathbf{x}_{0}^\\top \\\\\n",
    "\\mathbf{x}_{1}^\\top \\\\\n",
    " \\vdots \\\\\n",
    "\\mathbf{x}_{10}^\\top \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & x_{0} \\\\\n",
    "1 & x_{1} \\\\\n",
    " \\vdots \\\\\n",
    "1 & x_{10} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Prediction with linear models\n",
    "$$\n",
    "\\underbrace{\\mathbf{X}}_{(10,2)} \\underbrace{\\mathbf{w}}_{(2,1)} = \\begin{bmatrix}\n",
    "\\mathbf{x}_{0}^\\top \\\\\n",
    "\\mathbf{x}_{1}^\\top \\\\\n",
    " \\vdots \\\\\n",
    "\\mathbf{x}_{10}^\\top \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "b \\\\\n",
    "m \\\\\n",
    "\\end{bmatrix}= \\begin{bmatrix}\n",
    "1 & x_{0} \\\\\n",
    "1 & x_{1} \\\\\n",
    " \\vdots \\\\\n",
    "1 & x_{10} \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "b \\\\\n",
    "m \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "b + m\\;x_{0} \\\\\n",
    "b + m\\;x_{1} \\\\\n",
    " \\vdots \\\\\n",
    "b + m\\;x_{10} \\\\\n",
    "\\end{bmatrix} = \\underbrace{\\begin{bmatrix}\n",
    "f(x_{0}) \\\\\n",
    "f(x_{1}) \\\\\n",
    " \\vdots \\\\\n",
    "f(x_{10}) \\\\\n",
    "\\end{bmatrix}}_{(10,1)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the design matrix\n",
    "\n",
    "x_grid = np.linspace(-0,2,100) #change the number of points\n",
    "X =                     # [1, x] augmented matrix with bias term\n",
    "print(\"X: \", X.shape)\n",
    "print(X[:3,:])  # print first 3 rows\n",
    "\n",
    "b_best = mb_best[1]\n",
    "m_best = mb_best[0]\n",
    "w = np.array([[b_best],[m_best]])\n",
    "print(\"w = [b,m]:  \", w.shape)\n",
    "print(w)\n",
    "\n",
    "# prediction using matrix-vector multiplication\n",
    "y_pred = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "ax.scatter(x, y, label='data', s=75)\n",
    "ax.plot(x_grid, y_pred, c='k', marker='o',\n",
    "        label='model prediction', markersize=5)\n",
    "\n",
    "ax.set_xlabel(r'$x$', fontsize=20)\n",
    "ax.set_ylabel(r'$f(x)$', fontsize=20)\n",
    "ax.legend(fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Minimal, Colab-friendly ipywidgets demo for linear models: y = b + m x ---\n",
    "\n",
    "# Dense grid for the model line\n",
    "x_grid = np.linspace(0, 2, 100)\n",
    "\n",
    "# Grid over parameters\n",
    "m = np.linspace(-2., 5, 200)  # slope\n",
    "b = np.linspace(-2., 5, 200)  # intercept\n",
    "\n",
    "M, B = np.meshgrid(m, b)\n",
    "m_ = M.flatten()\n",
    "b_ = B.flatten()\n",
    "\n",
    "errors = []\n",
    "for mi, bi in zip(m_, b_):\n",
    "    w = np.array([[bi], [mi]])\n",
    "    # compute error for each (mi, bi,)\n",
    "    error = linear_model_mse(mi, bi, x, y)\n",
    "    errors.append(error)\n",
    "\n",
    "# reshape errors to grid\n",
    "errors = np.array(errors).reshape(M.shape)\n",
    "\n",
    "\n",
    "# ---- Plot function (like your draw(n=...)) ----\n",
    "def draw(b=0.0, m=0.0):\n",
    "    y_grid = linear_model(m, b, x_grid)\n",
    "    y_pred = linear_model(m, b, x)\n",
    "    error = y_pred - y\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    # left panel\n",
    "    ax1.plot(x_grid, y_grid, c='k')\n",
    "    for xi, yi in zip(x, y):\n",
    "        y_pred_i = linear_model(m, b, xi)\n",
    "        error_i = yi - y_pred_i\n",
    "        if error_i > 0:\n",
    "            ax1.vlines(xi, y_pred_i, y_pred_i + np.abs(error_i))\n",
    "        else:\n",
    "            ax1.vlines(xi, y_pred_i - np.abs(error_i), y_pred_i)\n",
    "    ax1.text(0.1, .9, f'f(x) = {m:.3f} x  + {b:.3f}',\n",
    "             fontsize=15, transform=ax1.transAxes)\n",
    "    ax1.scatter(x, y, s=45)\n",
    "    ax1.set_xlim(-0.5, 2.5)\n",
    "    ax1.set_ylabel(r'$y$', fontsize=20)\n",
    "    ax1.set_xlabel(r'$x$', fontsize=20)\n",
    "\n",
    "    # right panel\n",
    "    error_ = []\n",
    "    for xi, yi in zip(x, y):\n",
    "        y_pred_i = linear_model(m, b, xi)\n",
    "        error_i = yi - y_pred_i  # individual error\n",
    "        error_.append(error_i)\n",
    "    error_ = np.array(error_)\n",
    "    ax2.scatter(x, error_**2, color='k', s=55)\n",
    "    # ax2.scatter(x, np.abs(error_), color='r', s=55, marker='s',label=r'$|\\epsilon(x_i)|$')\n",
    "\n",
    "    ax2.set_ylabel(r'$\\epsilon^2(x)$', fontsize=15)\n",
    "    ax2.set_xlabel(r'$x$', fontsize=20)\n",
    "    # ax2.set_ylim(-0.5, 15.)\n",
    "    ax2.set_xlim(-0.5, 2.5)\n",
    "    \n",
    "    y_pred = linear_model(m, b, x) # only the data\n",
    "    error = linear_model_mse(m, b, x, y)\n",
    "    ax3.contourf(B, M, errors, levels=200, cmap='RdBu_r')\n",
    "    ax3.scatter([b], [m], color='k', s=100)  # current point\n",
    "    ax3.text(0.5,-1.5, r\"${\\cal L} = $\" + f\"{error:.3f}\", fontsize=18)\n",
    "    ax3.set_xlabel('b', fontsize=16)\n",
    "    ax3.set_ylabel('m', fontsize=16)\n",
    "\n",
    "\n",
    "# ---- Widgets: only m and b ----\n",
    "interact(\n",
    "    draw,\n",
    "    b=FloatSlider(value=0.0, min=-2., max=5., step=0.01, description=\"b\"),\n",
    "    m=FloatSlider(value=0.0, min=-2., max=5., step=0.01, description=\"m\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares\n",
    "The **least square** problem is synonym of the optimization algorithm for linear models with MSE. <br>\n",
    "Let's code the solution of the least squares problem using the matrix notation that we cover in class. <br>\n",
    "First, we will define the mean squared error as,\n",
    "$$\n",
    " {\\cal L}(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=i}^{N}  \\frac{1}{2} \\left (\\hat{y}_i - \\mathbf{w}^\\top x_i \\right )^2 = \\frac{1}{2N} \\left (\\underbrace{\\mathbf{Y}^\\top}_{(1,N)} \\underbrace{\\mathbf{Y}}_{(N,1)} - 2\\underbrace{\\mathbf{Y}^\\top}_{(1,N)}\\underbrace{\\mathbf{X}}_{(N,2)} \\underbrace{\\mathbf{w}}_{(2,1)}  + \\underbrace{\\mathbf{w}^\\top}_{(1,2)} \\underbrace{\\mathbf{X}^\\top}_{(2,N)}\\underbrace{\\mathbf{X}}_{(N,2)} \\underbrace{\\mathbf{w}}_{(2,1)} \\right )\n",
    "$$\n",
    "You can see the full derivation in [![Download PDF](https://img.shields.io/badge/Download_PDF-Click_Here-blue.svg)](https://github.com/ChemAI-Lab/Math4Chem/raw/main/website/Lecture_Notes/Notes/Linear_Regression.pdf)   \n",
    "\n",
    "Then we will proceed to compute the Jacobian of ${\\cal L}(\\mathbf{w})$ with respect to the parameters of the model, $\\mathbf{w}^\\top = [b,m]$,\n",
    "$$\n",
    " \\frac{\\partial }{\\partial \\mathbf{w}}{\\cal L}(\\mathbf{w}) = \\frac{1}{2N} \\frac{\\partial }{\\partial \\mathbf{w}}\\left (\\mathbf{Y}^\\top \\mathbf{Y} - 2\\mathbf{Y}^\\top\\mathbf{X} \\mathbf{w}  + \\mathbf{w}^\\top \\mathbf{X}^\\top\\mathbf{X} \\mathbf{w} \\right )\n",
    "$$\n",
    "which give us three unique terms,\n",
    "$$\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}} \\mathbf{Y}^\\top \\mathbf{Y} = \\mathbf{0} \\\\\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}} \\mathbf{Y}^\\top\\mathbf{X} \\mathbf{w} = \\mathbf{X}^\\top  \\mathbf{Y}\\\\\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}}\\mathbf{w}^\\top \\mathbf{X}^\\top\\mathbf{X} \\mathbf{w} = 2 \\mathbf{X}^\\top\\mathbf{X}\\mathbf{w}\n",
    "$$\n",
    "**References**: [Matrix CookBook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)<br>\n",
    "\n",
    "\n",
    "If we combine these three equations and set the Jacobian to zero, we  get,\n",
    "$$\n",
    " \\mathbf{0} = \\frac{1}{2N} \\left (\\mathbf{0} - 2\\mathbf{X}^\\top  \\mathbf{Y}  + 2\\mathbf{X}^\\top\\mathbf{X} \\mathbf{w} \\right ),\n",
    "$$\n",
    "where after cleaning it up, we get,\n",
    "$$\n",
    "\\mathbf{X}^\\top\\mathbf{X} \\mathbf{w}  = \\mathbf{X}^\\top  \\mathbf{Y}\n",
    "$$\n",
    "\n",
    "This matrix equation can be solved by inverting $\\mathbf{A} = \\mathbf{X}^\\top\\mathbf{X}$, \n",
    "$$\n",
    "\\mathbf{w}^*  =  \\mathbf{A}^{-1}\\mathbf{X}^\\top  \\mathbf{Y} = {(\\mathbf{X}^\\top\\mathbf{X})}^{-1} \\underbrace{\\mathbf{X}^\\top  \\mathbf{Y}}_{\\mathbf{z}}\n",
    "$$\n",
    "This is the central equation of this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2420990878.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[84], line 12\u001b[0;36m\u001b[0m\n\u001b[0;31m    A =\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "print('Training data')\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# design matrix\n",
    "X_tr = np.column_stack((np.ones_like(x),x))\n",
    "print('Design matrix')\n",
    "print(X_tr)\n",
    "\n",
    "# matrix A\n",
    "A = \n",
    "print('Matrix A')\n",
    "print(A)\n",
    "\n",
    "# vector z = Xt Y\n",
    "z = \n",
    "print('Vector z')\n",
    "print(z)\n",
    "\n",
    "\n",
    "# solve for w\n",
    "A_inv = np.linalg.inv(A) # inverse of A\n",
    "w_opt = \n",
    "print('Optimal solution')\n",
    "print(w_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grid = 25\n",
    "m_grid = np.linspace(-2, 4, n_grid)\n",
    "b_grid = np.linspace(-2, 4,  n_grid)\n",
    "m_, b_ = np.meshgrid(m_grid, b_grid)\n",
    "m_b = np.column_stack((m_.flatten(), b_.flatten()))\n",
    "\n",
    "total_error_ = []\n",
    "for abi in m_b:\n",
    "    m, b = abi[0], abi[1]\n",
    "    ei = linear_model_mse(m, b, x, y)\n",
    "    total_error_.append(ei)\n",
    "total_error_ = np.array(total_error_)\n",
    "total_error_ = total_error_.reshape(m_.shape)\n",
    "\n",
    "\n",
    "b_opt, a_opt = w_opt[0], w_opt[1]\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "CS = ax.contourf(a_, b_, total_error_, levels=50, cmap='RdBu_r')\n",
    "\n",
    "ax.scatter(a_opt, b_opt,\n",
    "           zorder=10, color='w', marker='v', s=225, label='optimal solution')\n",
    "ax.clabel(CS, inline=True, fontsize=10)\n",
    "ax.set_xlabel('a', fontsize=25)\n",
    "ax.set_ylabel('b', fontsize=25)\n",
    "ax.legend(loc='upper left', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction with these parameters is no different. \n",
    "\n",
    "$$\n",
    "f(\\mathbf{X}_{new}) = \\mathbf{X}_{new}^\\top \\mathbf{w}^* = \\mathbf{X}_{new}^\\top \\underbrace{{(\\mathbf{X}^\\top\\mathbf{X})}^{-1}\\mathbf{X}^\\top  \\mathbf{Y}}_{\\mathbf{w}^*} \\\\\n",
    "$$\n",
    "we do not need to compute ${(\\mathbf{X}^\\top\\mathbf{X})}^{-1}\\mathbf{X}^\\top  \\mathbf{Y}$ every time we do prediction, we only need to solve for it ones and store the vector $\\mathbf{w}^*$.\n",
    "\n",
    "<!-- f(\\mathbf{X}) = \\begin{bmatrix}\n",
    "1 & x_{0} \\\\\n",
    "1 & x_{1} \\\\\n",
    " \\vdots \\\\\n",
    "1 & x_{10} \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "b \\\\\n",
    "a\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "a\\;x_1+b \\\\\n",
    "a\\;x_2+b \\\\\n",
    "\\vdots \\\\\n",
    "a\\;x_n+b \\\\\n",
    "\\end{bmatrix} -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the design matrix\n",
    "\n",
    "x_grid = np.linspace(-0, 2, 100)  # change the number of points\n",
    "X = np.column_stack((np.ones_like(x_grid), x_grid))\n",
    "print(X.shape)\n",
    "print(X)\n",
    "\n",
    "# prediction\n",
    "y_pred = X @ w_opt\n",
    "\n",
    "\n",
    "# plot the result\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "ax.scatter(x, y, label='data', s=75)\n",
    "ax.plot(x_grid, y_pred, c='k', marker='o',\n",
    "        label='model prediction', markersize=5)\n",
    "\n",
    "ax.set_xlabel(r'$x$', fontsize=20)\n",
    "ax.set_ylabel(r'$f(x)$', fontsize=20)\n",
    "ax.legend(fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, m = w_opt[0], w_opt[1]\n",
    "draw(b, m) # plot with optimal parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem3pc3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
